{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 1: Setup & Mount Drive**\n",
        "\n",
        "* **What it does:** Connects Colab to your Google Drive so you can save files permanently."
      ],
      "metadata": {
        "id": "pyvZzmL7fgI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxVhN-_H7sq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b12085-ee96-47a0-ae00-8e05db1f348c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configuration paths\n",
        "ORIGINAL_DATA_DIR = '/content/drive/MyDrive/CNN/dataset'\n",
        "AUGMENTED_DATA_DIR = '/content/drive/MyDrive/CNN/augmented'\n",
        "PROCESSED_DATA_DIR = '/content/drive/MyDrive/CNN/features'\n",
        "MODELS_DIR = '/content/drive/MyDrive/CNN/saved_models'\n",
        "TRAIN_DIR = '/content/drive/MyDrive/CNN/split/train'\n",
        "VAL_DIR = '/content/drive/MyDrive/CNN/split/val'\n",
        "RESULTS_DIR = '/content/drive/MyDrive/CNN/results'\n",
        "MODEL_FILENAME = 'cnn_feature_extractor.pth'\n",
        "\n",
        "CLASS_NAMES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash','unknown']\n",
        "TARGET_IMAGES_PER_CLASS = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 2: Data Splitting**\n",
        "\n",
        "* **What it does:** Takes your raw images and splits them into Train (80%) and Validation (20%) folders."
      ],
      "metadata": {
        "id": "0IVQUwxDfrD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(original_dir, train_dir, val_dir, split_ratio=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Split dataset into train and validation sets BEFORE augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"SPLITTING DATASET INTO TRAIN / VALIDATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if os.path.exists(train_dir):\n",
        "        shutil.rmtree(train_dir)\n",
        "    if os.path.exists(val_dir):\n",
        "        shutil.rmtree(val_dir)\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_path = os.path.join(original_dir, class_name)\n",
        "        if not os.path.exists(class_path):\n",
        "            continue\n",
        "\n",
        "        images = [f for f in os.listdir(class_path)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "        train_imgs, val_imgs = train_test_split(\n",
        "            images, train_size=split_ratio, random_state=seed, shuffle=True\n",
        "        )\n",
        "\n",
        "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "\n",
        "        for img in train_imgs:\n",
        "            shutil.copy2(\n",
        "                os.path.join(class_path, img),\n",
        "                os.path.join(train_dir, class_name, img)\n",
        "            )\n",
        "\n",
        "        for img in val_imgs:\n",
        "            shutil.copy2(\n",
        "                os.path.join(class_path, img),\n",
        "                os.path.join(val_dir, class_name, img)\n",
        "            )\n",
        "\n",
        "        print(f\"[SPLIT] {class_name}: {len(train_imgs)} train | {len(val_imgs)} val\")\n",
        "\n",
        "    print(\"[OK] Dataset split completed\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    split_dataset(ORIGINAL_DATA_DIR, TRAIN_DIR, VAL_DIR, split_ratio=0.8, seed=42)"
      ],
      "metadata": {
        "id": "0WfqmjLS-fCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 3: Augmentation & Unknown Generation (Phase 1)**\n",
        "\n",
        "* **What it does:** Rotates/Zooms images to reach 500 per class. **Crucially**, it generates the \"Unknown\" class using the robust logic (Blur + Black boxes + Noise) we discussed."
      ],
      "metadata": {
        "id": "CU-ARQtcfwGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "ROTATION_RANGE = 30  # degrees\n",
        "BRIGHTNESS_RANGE = (0.7, 1.3)  # 70% to 130%\n",
        "ZOOM_RANGE = (0.8, 1.2)  # 80% to 120%\n",
        "FLIP_PROBABILITY = 0.5\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DATASET EXPLORATION\n",
        "# ============================================================================\n",
        "\n",
        "def explore_dataset(data_dir):\n",
        "    \"\"\"\n",
        "    Analyze the original dataset and print statistics\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"DATASET EXPLORATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    class_stats = {}\n",
        "    total_images = 0\n",
        "\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "\n",
        "        if not os.path.exists(class_path):\n",
        "            print(f\"WARNING: Folder '{class_name}' not found!\")\n",
        "            continue\n",
        "\n",
        "        # Count images\n",
        "        images = [f for f in os.listdir(class_path)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "        count = len(images)\n",
        "        total_images += count\n",
        "\n",
        "        class_stats[class_name] = {\n",
        "            'count': count,\n",
        "            'images': images\n",
        "        }\n",
        "\n",
        "        # Sample image dimensions\n",
        "        if count > 0:\n",
        "            sample_img_path = os.path.join(class_path, images[0])\n",
        "            sample_img = cv2.imread(sample_img_path)\n",
        "            if sample_img is not None:\n",
        "                height, width = sample_img.shape[:2]\n",
        "                class_stats[class_name]['sample_size'] = (width, height)\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"[STATS] Dataset Statistics:\")\n",
        "    print(f\"{'Class':<15} {'Count':<10} {'Percentage':<12} {'Sample Size'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for class_name in CLASS_NAMES:\n",
        "        if class_name in class_stats:\n",
        "            count = class_stats[class_name]['count']\n",
        "            percentage = (count / total_images) * 100 if total_images > 0 else 0\n",
        "            sample_size = class_stats[class_name].get('sample_size', 'N/A')\n",
        "            print(f\"{class_name:<15} {count:<10} {percentage:>6.2f}%      {sample_size}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'TOTAL':<15} {total_images:<10} 100.00%\")\n",
        "    print()\n",
        "\n",
        "    # Identify imbalances\n",
        "    if class_stats:\n",
        "        counts = [stats['count'] for stats in class_stats.values()]\n",
        "        max_count = max(counts)\n",
        "        min_count = min(counts)\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "        print(f\"[ANALYSIS] Class Imbalance Analysis:\")\n",
        "        print(f\"   Max class size: {max_count}\")\n",
        "        print(f\"   Min class size: {min_count}\")\n",
        "        print(f\"   Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
        "\n",
        "        if imbalance_ratio > 2:\n",
        "            print(f\"   Warning: Significant imbalance detected! Augmentation needed.\")\n",
        "        else:\n",
        "            print(f\"   Classes are relatively balanced.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
        "\n",
        "    return class_stats, total_images\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: AUGMENTATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "    \"\"\"Rotate image by given angle\"\"\"\n",
        "    height, width = image.shape[:2]\n",
        "    center = (width // 2, height // 2)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, matrix, (width, height),\n",
        "                             borderMode=cv2.BORDER_REFLECT)\n",
        "    return rotated\n",
        "\n",
        "\n",
        "def adjust_brightness(image, factor):\n",
        "    \"\"\"Adjust image brightness\"\"\"\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
        "    hsv[:, :, 2] = hsv[:, :, 2] * factor\n",
        "    hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)\n",
        "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
        "\n",
        "\n",
        "def zoom_image(image, zoom_factor):\n",
        "    \"\"\"Zoom in/out on image\"\"\"\n",
        "    height, width = image.shape[:2]\n",
        "    new_height, new_width = int(height * zoom_factor), int(width * zoom_factor)\n",
        "\n",
        "    # Resize\n",
        "    resized = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    # Crop or pad to original size\n",
        "    if zoom_factor > 1:  # Zoom in - crop center\n",
        "        start_y = (new_height - height) // 2\n",
        "        start_x = (new_width - width) // 2\n",
        "        result = resized[start_y:start_y+height, start_x:start_x+width]\n",
        "    else:  # Zoom out - pad\n",
        "        result = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        start_y = (height - new_height) // 2\n",
        "        start_x = (width - new_width) // 2\n",
        "        result[start_y:start_y+new_height, start_x:start_x+new_width] = resized\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def flip_image(image, flip_code):\n",
        "    \"\"\"Flip image horizontally (1) or vertically (0)\"\"\"\n",
        "    return cv2.flip(image, flip_code)\n",
        "\n",
        "\n",
        "def translate_image(image, tx, ty):\n",
        "    \"\"\"Translate image by tx, ty pixels\"\"\"\n",
        "    height, width = image.shape[:2]\n",
        "    matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    translated = cv2.warpAffine(image, matrix, (width, height),\n",
        "                                borderMode=cv2.BORDER_REFLECT)\n",
        "    return translated\n",
        "\n",
        "\n",
        "def augment_image(image):\n",
        "    \"\"\"\n",
        "    Apply random augmentation to a single image\n",
        "    Returns the augmented image\n",
        "    \"\"\"\n",
        "    img = image.copy()\n",
        "\n",
        "    # Random rotation\n",
        "    if random.random() > 0.3:\n",
        "        angle = random.uniform(-ROTATION_RANGE, ROTATION_RANGE)\n",
        "        img = rotate_image(img, angle)\n",
        "\n",
        "    # Random brightness\n",
        "    if random.random() > 0.3:\n",
        "        factor = random.uniform(BRIGHTNESS_RANGE[0], BRIGHTNESS_RANGE[1])\n",
        "        img = adjust_brightness(img, factor)\n",
        "\n",
        "    # Random zoom\n",
        "    if random.random() > 0.3:\n",
        "        zoom = random.uniform(ZOOM_RANGE[0], ZOOM_RANGE[1])\n",
        "        img = zoom_image(img, zoom)\n",
        "\n",
        "    # Random horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        img = flip_image(img, 1)\n",
        "\n",
        "    # Random translation\n",
        "    if random.random() > 0.4:\n",
        "        height, width = img.shape[:2]\n",
        "        tx = random.randint(-int(width*0.1), int(width*0.1))\n",
        "        ty = random.randint(-int(height*0.1), int(height*0.1))\n",
        "        img = translate_image(img, tx, ty)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: AUGMENTATION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def augment_class(class_name, original_images, original_dir, output_dir, target_count):\n",
        "    \"\"\"\n",
        "    Augment a single class to reach target_count images\n",
        "    \"\"\"\n",
        "    current_count = len(original_images)\n",
        "    needed = target_count - current_count\n",
        "\n",
        "    print(f\"[PROCESSING] Processing class: {class_name}\")\n",
        "    print(f\"   Original: {current_count} images\")\n",
        "    print(f\"   Target: {target_count} images\")\n",
        "    print(f\"   Need to generate: {needed} images\")\n",
        "\n",
        "    # Create output directory\n",
        "    class_output_dir = os.path.join(output_dir, class_name)\n",
        "    os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "    # Copy original images\n",
        "    print(f\"   - Copying original images...\")\n",
        "    for img_name in original_images:\n",
        "        src = os.path.join(original_dir, class_name, img_name)\n",
        "        dst = os.path.join(class_output_dir, img_name)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # Generate augmented images\n",
        "    if needed > 0:\n",
        "        print(f\"   - Generating {needed} augmented images...\")\n",
        "\n",
        "        aug_count = 0\n",
        "        with tqdm(total=needed, desc=f\"   Augmenting {class_name}\") as pbar:\n",
        "            while aug_count < needed:\n",
        "                # Randomly select an original image\n",
        "                original_img_name = random.choice(original_images)\n",
        "                original_img_path = os.path.join(original_dir, class_name, original_img_name)\n",
        "\n",
        "                # Read image\n",
        "                img = cv2.imread(original_img_path)\n",
        "                if img is None:\n",
        "                    continue\n",
        "\n",
        "                # Augment\n",
        "                aug_img = augment_image(img)\n",
        "\n",
        "                # Save with unique name\n",
        "                base_name = os.path.splitext(original_img_name)[0]\n",
        "                ext = os.path.splitext(original_img_name)[1]\n",
        "                aug_img_name = f\"{base_name}_aug_{aug_count}{ext}\"\n",
        "                aug_img_path = os.path.join(class_output_dir, aug_img_name)\n",
        "\n",
        "                cv2.imwrite(aug_img_path, aug_img)\n",
        "\n",
        "                aug_count += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "    final_count = len(os.listdir(class_output_dir))\n",
        "    print(f\"   [OK] Final count: {final_count} images\")\n",
        "\n",
        "    return final_count\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: UNKNOWN CLASS GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_unknown_class(augmented_dir, target_count=400):\n",
        "    \"\"\"\n",
        "    Generate Unknown class (ID: 6) from:\n",
        "    1. Heavily blurred existing images\n",
        "    2. Mixed/ambiguous samples\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GENERATING UNKNOWN CLASS (Class 6)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    unknown_dir = os.path.join(augmented_dir, 'unknown')\n",
        "    os.makedirs(unknown_dir, exist_ok=True)\n",
        "\n",
        "    all_images = []\n",
        "\n",
        "    # Collect all existing images\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_dir = os.path.join(augmented_dir, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            images = [os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
        "                     if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "            all_images.extend(images)\n",
        "\n",
        "    print(f\"\\n[STATS] Generating {target_count} unknown samples...\")\n",
        "    print(f\"   Strategy: Heavy blur + random transformations\")\n",
        "\n",
        "    with tqdm(total=target_count, desc=\"   Creating unknown samples\") as pbar:\n",
        "        for i in range(target_count):\n",
        "            # Randomly select an image\n",
        "            img_path = random.choice(all_images)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            # Apply heavy blur (this makes it \"unknown\")\n",
        "            blur_amount = random.randint(15, 35)\n",
        "            if blur_amount % 2 == 0:\n",
        "                blur_amount += 1\n",
        "            img = cv2.GaussianBlur(img, (blur_amount, blur_amount), 0)\n",
        "\n",
        "            # Optionally add more distortions\n",
        "            # STRATEGY 1: Cutout (Black blocks to hide features)\n",
        "            # This teaches model that \"missing info\" is not a specific class\n",
        "            for _ in range(random.randint(1, 4)):\n",
        "                x1 = random.randint(0, w)\n",
        "                y1 = random.randint(0, h)\n",
        "                cv2.rectangle(img, (x1, y1), (x1+60, y1+60), (0,0,0), -1)\n",
        "\n",
        "            # Pixel Noise (Texture destruction)\n",
        "            if random.random() > 0.5:\n",
        "                noise = np.random.normal(0, 40, img.shape).astype(np.uint8)\n",
        "                img = cv2.add(img, noise)\n",
        "\n",
        "            # Save\n",
        "            unknown_img_name = f\"unknown_{i:04d}.jpg\"\n",
        "            unknown_img_path = os.path.join(unknown_dir, unknown_img_name)\n",
        "            cv2.imwrite(unknown_img_path, img)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    final_count = len(os.listdir(unknown_dir))\n",
        "    print(f\"\\n   [OK] Generated {final_count} unknown samples\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    return final_count\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_samples(augmented_dir):\n",
        "    \"\"\"\n",
        "    Display sample images from each class\n",
        "    \"\"\"\n",
        "    print(\"\\n[INFO] Visualizing samples from each class...\")\n",
        "\n",
        "    all_classes = CLASS_NAMES + ['unknown']\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, class_name in enumerate(all_classes):\n",
        "        class_dir = os.path.join(augmented_dir, class_name)\n",
        "\n",
        "        if os.path.exists(class_dir):\n",
        "            images = [f for f in os.listdir(class_dir)\n",
        "                     if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "            if images:\n",
        "                # Select a random image\n",
        "                sample_img_name = random.choice(images)\n",
        "                sample_img_path = os.path.join(class_dir, sample_img_name)\n",
        "                img = cv2.imread(sample_img_path)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                axes[idx].imshow(img)\n",
        "                axes[idx].set_title(f\"{class_name.capitalize()} ({len(images)} images)\")\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "    # Hide last empty subplot\n",
        "    if len(all_classes) < len(axes):\n",
        "        axes[-1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dataset_samples.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"   [OK] Saved visualization to 'dataset_samples.png'\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_class_distribution(augmented_dir):\n",
        "    \"\"\"\n",
        "    Plot bar chart of class distribution\n",
        "    \"\"\"\n",
        "    print(\"\\n[CHART] Creating class distribution chart...\")\n",
        "\n",
        "    all_classes = CLASS_NAMES + ['unknown']\n",
        "    counts = []\n",
        "\n",
        "    for class_name in all_classes:\n",
        "        class_dir = os.path.join(augmented_dir, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            count = len([f for f in os.listdir(class_dir)\n",
        "                        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
        "            counts.append(count)\n",
        "        else:\n",
        "            counts.append(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(range(len(all_classes)), counts, color='steelblue', alpha=0.8)\n",
        "    plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
        "    plt.title('Augmented Dataset Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(range(len(all_classes)),\n",
        "               [c.capitalize() for c in all_classes],\n",
        "               rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add count labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(count)}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"   [OK] Saved chart to 'class_distribution.png'\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution pipeline\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 25)\n",
        "    print(\"PHASE 1: DATA PREPARATION & AUGMENTATION\")\n",
        "    print(\"=\" * 25 + \"\\n\")\n",
        "\n",
        "    # Step 1: Explore original dataset\n",
        "    class_stats, total_original = explore_dataset(TRAIN_DIR)\n",
        "\n",
        "    if not class_stats:\n",
        "        print(\"[ERROR] Error: No valid dataset found!\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Create output directory\n",
        "    os.makedirs(AUGMENTED_DATA_DIR, exist_ok=True)\n",
        "\n",
        "    # Step 3: Augment each class\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"AUGMENTATION PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    augmented_stats = {}\n",
        "\n",
        "    for class_name in CLASS_NAMES:\n",
        "        if class_name in class_stats:\n",
        "            original_images = class_stats[class_name]['images']\n",
        "            final_count = augment_class(\n",
        "                class_name,\n",
        "                original_images,\n",
        "                ORIGINAL_DATA_DIR,\n",
        "                AUGMENTED_DATA_DIR,\n",
        "                TARGET_IMAGES_PER_CLASS\n",
        "            )\n",
        "            augmented_stats[class_name] = final_count\n",
        "\n",
        "    # Step 4: Generate Unknown class\n",
        "    unknown_count = generate_unknown_class(AUGMENTED_DATA_DIR, target_count=400)\n",
        "    augmented_stats['unknown'] = unknown_count\n",
        "\n",
        "    # Step 5: Print final summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_augmented = sum(augmented_stats.values())\n",
        "    increase_percentage = ((total_augmented - total_original) / total_original) * 100\n",
        "\n",
        "    print(f\"\\n[STATS] Final Dataset Statistics:\")\n",
        "    print(f\"{'Class':<15} {'Original':<12} {'Augmented':<12} {'Increase'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Iterate only over classes that were present in the original dataset\n",
        "    # The 'unknown' class was not in the original dataset, so exclude it here\n",
        "    for class_name in [cn for cn in CLASS_NAMES if cn != 'unknown']:\n",
        "        original = class_stats[class_name]['count']\n",
        "        augmented = augmented_stats.get(class_name, 0)\n",
        "        increase = augmented - original\n",
        "        print(f\"{class_name:<15} {original:<12} {augmented:<12} +{increase}\")\n",
        "\n",
        "    # Print the 'unknown' class details separately, as it had 0 original images\n",
        "    print(f\"{'unknown':<15} {0:<12} {unknown_count:<12} +{unknown_count}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'TOTAL':<15} {total_original:<12} {total_augmented:<12} \"\n",
        "          f\"+{total_augmented - total_original} ({increase_percentage:.1f}%)\")\n",
        "\n",
        "    if increase_percentage >= 30:\n",
        "        print(f\"\\n[SUCCESS] Dataset increased by {increase_percentage:.1f}% (target: 30%)\")\n",
        "    else:\n",
        "        print(f\"\\n[WARNING] Dataset only increased by {increase_percentage:.1f}% \"\n",
        "              f\"(target: 30%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VemH0KcAAIzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 4: CNN Feature Extraction (Phase 2)**\n",
        "\n",
        "* **What it does:** Downloads ResNet50, trains it briefly on your data to learn features, then saves the \"Feature Vectors\" (numerical representations of images) to `.npy` files."
      ],
      "metadata": {
        "id": "onSjpb6wfzLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Phase 2 :CNN Feature Extractor\n",
        "\n",
        "This file is a feature-extraction pipeline with a\n",
        "compact PyTorch CNN training script that uses a pretrained ResNet50\n",
        "to learn features (and save weights). The implementation is organized\n",
        "into configuration, helpers, training/validation loops, and a main\n",
        "pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "# Training configuration\n",
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR = 3e-5\n",
        "patience = 3\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "\n",
        "FEATURES_DIR = os.environ.get('FEATURES_DIR', '/content/drive/MyDrive/CNN//features')\n",
        "\n",
        "# TRANSFORMS & DATASET HELPERS\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def is_valid_image(path: str) -> bool:\n",
        "    \"\"\"Quickly check an image file for basic integrity using PIL.\n",
        "\n",
        "    Returns True when the file opens and verifies; False otherwise.\n",
        "    Prints a short message for skipped files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with Image.open(path) as img:\n",
        "            img.verify()\n",
        "        return True\n",
        "    except Exception:\n",
        "        print(\"Skipping corrupted file:\", path)\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Return torch.device: CUDA if available and not forced off, else CPU.\n",
        "\n",
        "    Honor env var `FORCE_CPU=1` to force CPU even when CUDA is available.\n",
        "    \"\"\"\n",
        "    force_cpu = os.environ.get('FORCE_CPU', '') in ('1', 'true', 'True')\n",
        "    if not force_cpu and torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "def prepare_datasets(dataset_path: str):\n",
        "    \"\"\"Load ImageFolder, filter invalid files, and split into train/val.\n",
        "\n",
        "    Returns train_loader, val_loader and number of classes.\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform, is_valid_file=is_valid_image)\n",
        "    val_dataset   = datasets.ImageFolder(root=VAL_DIR, transform=transform, is_valid_file=is_valid_image)\n",
        "    num_classes = len(train_dataset.classes)\n",
        "    print(\"Classes:\", train_dataset.classes)\n",
        "    print(f\"Training images: {len(train_dataset)}, Validation images: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader, num_classes\n",
        "\n",
        "\n",
        "def extract_and_cache_features(dataset_path: str = TRAIN_DIR, weights_path: str = None, overwrite: bool = False):\n",
        "    \"\"\"Extract pooled CNN features using ResNet backbone and cache to .npy files.\n",
        "\n",
        "    Saves: X_train.npy, X_val.npy, X_test.npy, y_train.npy, y_val.npy, y_test.npy\n",
        "    under `FEATURES_DIR` (default 'data/features'). Splits dataset using TRAIN_RATIO.\n",
        "    \"\"\"\n",
        "    os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "\n",
        "    # If files exist and not overwriting, skip\n",
        "    expected = [os.path.join(FEATURES_DIR, n) for n in (\n",
        "        'X_train.npy','X_val.npy','X_test.npy','y_train.npy','y_val.npy','y_test.npy')]\n",
        "    if not overwrite and all(os.path.exists(p) for p in expected):\n",
        "        print(f\"Feature files already exist in {FEATURES_DIR}, use overwrite=True to regenerate.\")\n",
        "        return\n",
        "\n",
        "    # Build dataset\n",
        "    full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform, is_valid_file=is_valid_image)\n",
        "    if len(full_dataset) == 0:\n",
        "        raise RuntimeError(f\"No images found in {dataset_path}\")\n",
        "\n",
        "    device = get_device()\n",
        "    print('Using device for feature extraction:', device)\n",
        "\n",
        "    # Load model and weights if available\n",
        "    try:\n",
        "        model = models.resnet50(pretrained=False)\n",
        "    except Exception:\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    # Try to load provided weights or default saved model\n",
        "    if weights_path is None:\n",
        "        weights_path = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
        "\n",
        "    if weights_path and os.path.exists(weights_path):\n",
        "        try:\n",
        "            state = torch.load(weights_path, map_location=device)\n",
        "            model.load_state_dict(state)\n",
        "            print(f\"Loaded CNN weights from {weights_path}\")\n",
        "        except Exception:\n",
        "            # partial load: update matching shapes\n",
        "            sd = model.state_dict()\n",
        "            filtered = {k: v for k, v in state.items() if k in sd and sd[k].shape == v.shape}\n",
        "            sd.update(filtered)\n",
        "            model.load_state_dict(sd)\n",
        "            print(\"Loaded subset of weights (partial load)\")\n",
        "    else:\n",
        "        try:\n",
        "            model = models.resnet50(pretrained=True)\n",
        "            print('No saved weights found; using ImageNet pretrained backbone')\n",
        "        except Exception:\n",
        "            print('Warning: using randomly initialized ResNet (no pretrained weights)')\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Feature extractor: everything except the final fc layer\n",
        "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "\n",
        "    def _compute(ds):\n",
        "\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "        feats = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labs in tqdm(loader, desc='Extracting features'):\n",
        "                imgs = imgs.to(device)\n",
        "                out = feature_extractor(imgs)\n",
        "                out = out.reshape(out.size(0), -1).cpu().numpy()\n",
        "                feats.append(out)\n",
        "                labels.append(labs.numpy())\n",
        "        if feats:\n",
        "            return np.vstack(feats), np.concatenate(labels)\n",
        "        return np.zeros((0,0)), np.array([])\n",
        "\n",
        "    train_ds = datasets.ImageFolder(root=TRAIN_DIR, transform=transform, is_valid_file=is_valid_image)\n",
        "    val_ds = datasets.ImageFolder(root=VAL_DIR, transform=transform, is_valid_file=is_valid_image)\n",
        "    X_train, y_train = _compute(train_ds)\n",
        "    X_val, y_val = _compute(val_ds)\n",
        "\n",
        "    # Normalize features (fit on training set) and save scaler\n",
        "    if X_train.size == 0:\n",
        "        print(\"No features extracted; skipping save.\")\n",
        "        return\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val) if X_val.size else X_val\n",
        "\n",
        "    # Ensure model dir exists and save scaler\n",
        "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "    scaler_path = os.path.join(MODELS_DIR, 'feature_scaler.pkl')\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    # Save scaled features and labels\n",
        "    np.save(os.path.join(FEATURES_DIR, 'X_train.npy'), X_train_scaled)\n",
        "    np.save(os.path.join(FEATURES_DIR, 'X_val.npy'), X_val_scaled)\n",
        "    np.save(os.path.join(FEATURES_DIR, 'y_train.npy'), y_train)\n",
        "    np.save(os.path.join(FEATURES_DIR, 'y_val.npy'), y_val)\n",
        "\n",
        "    print(f\"Saved scaled features to {FEATURES_DIR}: X_train={X_train_scaled.shape}, X_val={X_val_scaled.shape}\")\n",
        "    print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "\n",
        "# MODEL, LOSS, OPTIMIZER\n",
        "\n",
        "def build_model(num_classes: int, device: torch.device):\n",
        "    \"\"\"Create a pretrained ResNet50 and replace the final head.\n",
        "\n",
        "    The function returns the model moved to the given device.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = models.resnet50(pretrained=True)\n",
        "    except Exception:\n",
        "        # Fallback for torchvision versions where 'pretrained' is deprecated\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    #FREEZE ENTIRE BACKBONE\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace classifier head with the number of classes we have\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "    nn.Linear(in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "    #  Unfreeze classifier head\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "# TRAIN / VALIDATION LOOPS\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, epoch_num, print_every=10):\n",
        "    \"\"\"Train for one epoch and return avg loss and accuracy.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(tqdm(loader, desc=f\"Epoch {epoch_num+1}\")):\n",
        "        try:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if (i + 1) % print_every == 0 or (i + 1) == len(loader):\n",
        "                timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "                progress = (i + 1) / len(loader) * 100\n",
        "                avg_loss = running_loss / (i + 1)\n",
        "                avg_acc = correct / total if total > 0 else 0\n",
        "                batch_acc = (preds == labels).sum().item() / len(labels)\n",
        "                gpu_mem = torch.cuda.memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else 0\n",
        "                print(f\"[{timestamp}] Epoch {epoch_num+1} | Batch {i+1}/{len(loader)} ({progress:.1f}%) | \"\n",
        "                      f\"Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f} | Batch Acc: {batch_acc:.4f} | \"\n",
        "                      f\"Avg Acc: {avg_acc:.4f} | GPU Mem: {gpu_mem:.1f}MB\", flush=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping batch {i+1} due to error: {e}\", flush=True)\n",
        "            continue\n",
        "\n",
        "    return running_loss / len(loader), correct / total if total > 0 else 0\n",
        "\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    \"\"\"Evaluate model on validation loader and return accuracy.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"[Validation] Batch {i+1}/{len(loader)} | Batch Acc: {(preds==labels).sum().item()/len(labels):.4f} | Total Acc So Far: {correct/total:.4f}\", flush=True)\n",
        "\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "\n",
        "# SAVE / MAIN PIPELINE\n",
        "\n",
        "\n",
        "def save_model_state(model, model_dir: str, filename: str):\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    path = os.path.join(model_dir, filename)\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"CNN weights saved to: {path}\")\n",
        "\n",
        "\n",
        "def main(dataset_path: str = TRAIN_DIR):\n",
        "    print(\"\\n\" + \"=\" * 25)\n",
        "    print(\"PHASE 2: CNN Feature Extractor\")\n",
        "    print(\"=\" * 25 + \"\\n\")\n",
        "\n",
        "    train_loader, val_loader, num_classes = prepare_datasets(dataset_path)\n",
        "\n",
        "    if len(train_loader.dataset) == 0 and len(val_loader.dataset) == 0:\n",
        "        print(\"No images found. Check DATASET_PATH.\")\n",
        "        return\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    model = build_model(num_classes, device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, print_every=10)\n",
        "        val_acc = validate(model, val_loader, device)\n",
        "\n",
        "        print(f\"--- Epoch {epoch+1} Summary ---\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\\n\", flush=True)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          epochs_without_improvement = 0\n",
        "          save_model_state(model, MODELS_DIR, MODEL_FILENAME)\n",
        "        else:\n",
        "          epochs_without_improvement += 1\n",
        "          if epochs_without_improvement >= patience:\n",
        "              print(f\"No improvement for {patience} epochs. Stopping early.\")\n",
        "              break\n",
        "\n",
        "    print(\"\\nTraining complete.\")\n",
        "    extract_and_cache_features(dataset_path=TRAIN_DIR, weights_path=os.path.join(MODELS_DIR, MODEL_FILENAME))\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "3k7IqrnLAaWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 5: KNN Training**\n",
        "\n",
        "* **What it does:** Loads the feature vectors and trains a K-Nearest Neighbors classifier."
      ],
      "metadata": {
        "id": "oeXEsKzFf5Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "class KNNTrainer:\n",
        "    \"\"\"KNN model training with hyperparameter tuning\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir=\"/content/drive/MyDrive/CNN/features\", model_dir=\"/content/drive/MyDrive/CNN/saved_models\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load pre-extracted features from Phase 2\"\"\"\n",
        "        try:\n",
        "            X_train = np.load(self.data_dir / \"X_train.npy\")\n",
        "            X_val = np.load(self.data_dir / \"X_val.npy\")\n",
        "            y_train = np.load(self.data_dir / \"y_train.npy\")\n",
        "            y_val = np.load(self.data_dir / \"y_val.npy\")\n",
        "\n",
        "            print(f\"[OK] Data loaded - Features shape: {X_train.shape}\")\n",
        "            return X_train, X_val, y_train, y_val\n",
        "        except FileNotFoundError as e:\n",
        "            raise FileNotFoundError(f\"Features not found. Run phase2_feature_extraction.py first. Error: {e}\")\n",
        "\n",
        "    def train_with_grid_search(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"Hyperparameter tuning for KNN\"\"\"\n",
        "        param_grid = {\n",
        "            'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan']\n",
        "        }\n",
        "\n",
        "        print(\"\\n[TUNING] Tuning KNN hyperparameters...\")\n",
        "        knn = KNeighborsClassifier()\n",
        "        grid_search = GridSearchCV(knn, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"\\n[OK] Best Parameters: {grid_search.best_params_}\")\n",
        "        print(f\"[OK] Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        return grid_search.best_estimator_, grid_search.best_params_\n",
        "\n",
        "    def evaluate_model(self, model, X_train, X_val, y_train, y_val, class_names=None):\n",
        "        \"\"\"Evaluate KNN on all sets\"\"\"\n",
        "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
        "        test_acc = accuracy_score(y_val, model.predict(X_val))\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"KNN MODEL PERFORMANCE\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Training Accuracy:   {train_acc:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        print(\"\\nClassification Report (Test Set):\")\n",
        "        # Pass class_names excluding 'unknown' for KNN evaluation\n",
        "        print(classification_report(y_val, model.predict(X_val), target_names=class_names[:-1]))\n",
        "\n",
        "        cm = confusion_matrix(y_val, model.predict(X_val))\n",
        "\n",
        "        return {\n",
        "            'train_accuracy': float(train_acc),\n",
        "            'val_accuracy': float(val_acc),\n",
        "            'confusion_matrix': cm.tolist()\n",
        "        }\n",
        "\n",
        "    def save_model(self, model, best_params, metrics, feature_dim):\n",
        "        \"\"\"Save KNN model and config\"\"\"\n",
        "        with open(self.model_dir / \"knn_model.pkl\", \"wb\") as f:\n",
        "            pickle.dump(model, f)\n",
        "\n",
        "        config = {\n",
        "            'model_type': 'KNN',\n",
        "            'best_params': best_params,\n",
        "            'metrics': metrics,\n",
        "            'feature_dimension': feature_dim\n",
        "        }\n",
        "\n",
        "        with open(self.model_dir / \"knn_config.json\", \"w\") as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "\n",
        "        print(f\"[OK] KNN model saved to {self.model_dir / 'knn_model.pkl'}\")\n",
        "        print(f\"[OK] Config saved to {self.model_dir / 'knn_config.json'}\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Full training pipeline\"\"\"\n",
        "        # Load features\n",
        "        X_train, X_val, y_train, y_val= self.load_data()\n",
        "\n",
        "        # Train with hyperparameter tuning\n",
        "        best_model, best_params = self.train_with_grid_search(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Evaluate\n",
        "        metrics = self.evaluate_model(best_model, X_train, X_val,\n",
        "                                     y_train, y_val,\n",
        "                                     CLASS_NAMES)\n",
        "\n",
        "        # Save\n",
        "        self.save_model(best_model, best_params, metrics, X_train.shape[1])\n",
        "\n",
        "trainer = KNNTrainer()\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_I-2K6DeAv0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 6: SVM Training (Phase 3)**\n",
        "\n",
        "* **What it does:** Trains the SVM and calculates the probability threshold for rejection."
      ],
      "metadata": {
        "id": "dNDsSLNJgden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_processed_data():\n",
        "    \"\"\"\n",
        "    Load preprocessed features and labels\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LOADING PREPROCESSED DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Load training data\n",
        "        X_train = np.load(os.path.join(PROCESSED_DATA_DIR, 'X_train.npy'))\n",
        "        y_train = np.load(os.path.join(PROCESSED_DATA_DIR, 'y_train.npy'))\n",
        "\n",
        "        # Load validation data\n",
        "        X_val = np.load(os.path.join(PROCESSED_DATA_DIR, 'X_val.npy'))\n",
        "        y_val = np.load(os.path.join(PROCESSED_DATA_DIR, 'y_val.npy'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"\\n[OK] Data loaded successfully!\")\n",
        "        print(f\"\\n[INFO] Dataset Information:\")\n",
        "        print(f\"   Training set:   {X_train.shape[0]:>5} samples x {X_train.shape[1]:>5} features\")\n",
        "        print(f\"   Validation set: {X_val.shape[0]:>5} samples x {X_val.shape[1]:>5} features\")\n",
        "\n",
        "        # Class distribution\n",
        "        print(f\"\\n[DISTRIBUTION] Training Set Class Distribution:\")\n",
        "        for class_id, class_name in enumerate(CLASS_NAMES):\n",
        "            count = np.sum(y_train == class_id)\n",
        "            percentage = (count / len(y_train)) * 100\n",
        "            print(f\"   {class_name:<12} {count:>5} samples ({percentage:>5.1f}%)\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        return X_train, y_train, X_val, y_val\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n[ERROR] Error: Could not find preprocessed data files!\")\n",
        "        print(f\"   Missing file: {e.filename}\")\n",
        "        print(f\"   Please run Phase 2 (feature extraction) first.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING\n",
        "# ============================================================================\n",
        "\n",
        "def tune_svm_hyperparameters(X_train, y_train, quick_mode=False):\n",
        "    \"\"\"\n",
        "    Find optimal SVM hyperparameters using GridSearchCV\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training labels\n",
        "        quick_mode: If True, use smaller grid for faster tuning\n",
        "\n",
        "    Returns:\n",
        "        best_params: Dictionary of best hyperparameters\n",
        "        grid_search: Fitted GridSearchCV object\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"SVM HYPERPARAMETER TUNING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if quick_mode:\n",
        "        print(\"\\n[FAST] Quick Mode: Testing limited parameter combinations\")\n",
        "        param_grid = {\n",
        "            'C': [1, 10, 100],\n",
        "            'gamma': ['scale', 0.001, 0.01],\n",
        "            'kernel': ['rbf', 'poly']\n",
        "        }\n",
        "    else:\n",
        "        print(\"\\n[FULL] Full Mode: Comprehensive parameter search\")\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10, 50, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "            'kernel': ['rbf', 'poly', 'linear']\n",
        "        }\n",
        "\n",
        "    print(f\"\\n[PARAMS] Parameter Grid:\")\n",
        "    for param, values in param_grid.items():\n",
        "        print(f\"   {param:<10} {values}\")\n",
        "\n",
        "    total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
        "    print(f\"\\n[COUNT] Total combinations to test: {total_combinations}\")\n",
        "    print(f\"   With 5-fold CV: {total_combinations * 5} model fits\")\n",
        "\n",
        "    # Estimate time\n",
        "    if quick_mode:\n",
        "        est_time = \"5-10 minutes\"\n",
        "    else:\n",
        "        est_time = \"15-30 minutes\"\n",
        "    print(f\"   Estimated time: {est_time}\")\n",
        "\n",
        "    # Create SVM with probability estimates\n",
        "    svm = SVC(probability=True, random_state=42, cache_size=1000)\n",
        "\n",
        "    # GridSearchCV with cross-validation\n",
        "    print(f\"\\n[START] Starting grid search...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # how to show every training in every parameter in every fold\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=svm,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,  # 5-fold cross-validation\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,  # Use all CPU cores\n",
        "        verbose=2,\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n[OK] Grid search complete!\")\n",
        "    print(f\"   Time elapsed: {elapsed_time/60:.1f} minutes\")\n",
        "    print(f\"\\n[BEST] Best Parameters:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"   {param:<10} {value}\")\n",
        "\n",
        "    print(f\"\\n[SCORE] Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Show top 5 parameter combinations\n",
        "    print(f\"\\n[TOP] Top 5 Parameter Combinations:\")\n",
        "    results = grid_search.cv_results_\n",
        "    indices = np.argsort(results['mean_test_score'])[::-1][:5]\n",
        "\n",
        "    for i, idx in enumerate(indices, 1):\n",
        "        params = results['params'][idx]\n",
        "        score = results['mean_test_score'][idx]\n",
        "        std = results['std_test_score'][idx]\n",
        "        print(f\"   {i}. Score: {score:.4f} (+/- {std:.4f})\")\n",
        "        print(f\"      Params: C={params['C']}, gamma={params['gamma']}, kernel={params['kernel']}\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return grid_search.best_params_, grid_search\n",
        "\n",
        "\n",
        "def analyze_hyperparameter_impact(grid_search):\n",
        "    \"\"\"\n",
        "    Visualize the impact of different hyperparameters\n",
        "    \"\"\"\n",
        "    print(\"\\n[ANALYSIS] Creating hyperparameter impact visualizations...\")\n",
        "\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Extract results for RBF kernel\n",
        "    rbf_mask = np.array([p['kernel'] == 'rbf' for p in results['params']])\n",
        "    if rbf_mask.any():\n",
        "        rbf_results = {\n",
        "            'C': [],\n",
        "            'gamma': [],\n",
        "            'score': []\n",
        "        }\n",
        "\n",
        "        for i, mask in enumerate(rbf_mask):\n",
        "            if mask:\n",
        "                rbf_results['C'].append(results['params'][i]['C'])\n",
        "                gamma_val = results['params'][i]['gamma']\n",
        "                # Convert 'scale' and 'auto' to numeric for plotting\n",
        "                if gamma_val == 'scale':\n",
        "                    gamma_val = 0.0001  # Placeholder\n",
        "                elif gamma_val == 'auto':\n",
        "                    gamma_val = 0.0005  # Placeholder\n",
        "                rbf_results['gamma'].append(gamma_val)\n",
        "                rbf_results['score'].append(results['mean_test_score'][i])\n",
        "\n",
        "        # Create heatmap\n",
        "        if rbf_results['C']:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "            # Prepare data for heatmap\n",
        "            C_values = sorted(list(set(rbf_results['C'])))\n",
        "            gamma_values = sorted(list(set(rbf_results['gamma'])))\n",
        "\n",
        "            heatmap_data = np.zeros((len(gamma_values), len(C_values)))\n",
        "\n",
        "            for c, g, s in zip(rbf_results['C'], rbf_results['gamma'], rbf_results['score']):\n",
        "                i = gamma_values.index(g)\n",
        "                j = C_values.index(c)\n",
        "                heatmap_data[i, j] = s\n",
        "\n",
        "            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                       xticklabels=C_values, yticklabels=[f'{g:.4f}' for g in gamma_values],\n",
        "                       cbar_kws={'label': 'CV Accuracy'}, ax=ax)\n",
        "\n",
        "            ax.set_xlabel('C (Regularization)', fontweight='bold')\n",
        "            ax.set_ylabel('Gamma', fontweight='bold')\n",
        "            ax.set_title('SVM Hyperparameter Impact (RBF Kernel)', fontweight='bold', fontsize=14)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(RESULTS_DIR, 'hyperparameter_heatmap.png'), dpi=150)\n",
        "            print(\"   [OK] Saved: hyperparameter_heatmap.png\")\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SVM TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "def train_final_svm(X_train, y_train, best_params):\n",
        "    \"\"\"\n",
        "    Train final SVM model with best parameters\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING FINAL SVM MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(f\"\\n[CONFIG] Configuration:\")\n",
        "    print(f\"   Kernel:  {best_params['kernel']}\")\n",
        "    print(f\"   C:       {best_params['C']}\")\n",
        "    print(f\"   Gamma:   {best_params['gamma']}\")\n",
        "    print(f\"   Probability: True (for confidence scores)\")\n",
        "\n",
        "    # Create and train SVM\n",
        "    print(f\"\\n[TRAINING] Training SVM on full training set...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    svm_model = SVC(\n",
        "        kernel=best_params['kernel'],\n",
        "        C=best_params['C'],\n",
        "        gamma=best_params['gamma'],\n",
        "        probability=True,  # Enable probability estimates\n",
        "        random_state=42,\n",
        "        cache_size=1000,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"[OK] Training complete!\")\n",
        "    print(f\"   Time: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"   Support vectors: {svm_model.n_support_.sum()}\")\n",
        "    print(f\"   Support vectors per class: {svm_model.n_support_}\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return svm_model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UNKNOWN CLASS DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "def predict_with_rejection(model, X, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Predict with confidence-based rejection for unknown class\n",
        "\n",
        "    Args:\n",
        "        model: Trained SVM model\n",
        "        X: Feature array\n",
        "        threshold: Confidence threshold (predictions below this -> unknown)\n",
        "\n",
        "    Returns:\n",
        "        predictions: Array of predicted classes (with unknown=6)\n",
        "        confidences: Array of confidence scores\n",
        "    \"\"\"\n",
        "    # Get probability predictions\n",
        "    probabilities = model.predict_proba(X)\n",
        "\n",
        "    # Get max probability and predicted class for each sample\n",
        "    max_probs = probabilities.max(axis=1)\n",
        "    predicted_classes = probabilities.argmax(axis=1)\n",
        "\n",
        "    # Apply rejection: if confidence < threshold, classify as unknown (6)\n",
        "    predictions = predicted_classes.copy()\n",
        "    predictions[max_probs < threshold] = 6  # Unknown class\n",
        "\n",
        "    return predictions, max_probs\n",
        "\n",
        "\n",
        "def find_optimal_threshold(model, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Find optimal confidence threshold for unknown class detection\n",
        "\n",
        "    Args:\n",
        "        model: Trained SVM model\n",
        "        X_val: Validation features\n",
        "        y_val: Validation labels\n",
        "\n",
        "    Returns:\n",
        "        optimal_threshold: Best threshold value\n",
        "        threshold_results: Dictionary with results for different thresholds\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINDING OPTIMAL REJECTION THRESHOLD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    thresholds = np.arange(0.3, 0.9, 0.05)\n",
        "    results = {\n",
        "        'threshold': [],\n",
        "        'known_accuracy': [],\n",
        "        'unknown_recall': [],\n",
        "        'overall_accuracy': [],\n",
        "        'f1_weighted': []\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[SEARCH] Testing {len(thresholds)} different thresholds...\")\n",
        "\n",
        "    for threshold in tqdm(thresholds, desc=\"   Testing thresholds\"):\n",
        "        predictions, confidences = predict_with_rejection(model, X_val, threshold)\n",
        "\n",
        "        # Calculate metrics\n",
        "        overall_acc = accuracy_score(y_val, predictions)\n",
        "\n",
        "        # Known classes accuracy (excluding unknown)\n",
        "        known_mask = y_val != 6\n",
        "        if known_mask.any():\n",
        "            known_acc = accuracy_score(y_val[known_mask], predictions[known_mask])\n",
        "        else:\n",
        "            known_acc = 0.0\n",
        "\n",
        "        # Unknown class recall\n",
        "        unknown_mask = y_val == 6\n",
        "        if unknown_mask.any():\n",
        "            unknown_recall = accuracy_score(y_val[unknown_mask], predictions[unknown_mask])\n",
        "        else:\n",
        "            unknown_recall = 0.0\n",
        "\n",
        "        # Weighted F1\n",
        "        _, _, f1, _ = precision_recall_fscore_support(y_val, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        results['threshold'].append(threshold)\n",
        "        results['known_accuracy'].append(known_acc)\n",
        "        results['unknown_recall'].append(unknown_recall)\n",
        "        results['overall_accuracy'].append(overall_acc)\n",
        "        results['f1_weighted'].append(f1)\n",
        "\n",
        "    # Find optimal threshold (maximize overall accuracy)\n",
        "    optimal_idx = np.argmax(results['overall_accuracy'])\n",
        "    optimal_threshold = results['threshold'][optimal_idx]\n",
        "\n",
        "    print(f\"\\n[TARGET] Optimal Threshold: {optimal_threshold:.2f}\")\n",
        "    print(f\"\\n[PERF] Performance at Optimal Threshold:\")\n",
        "    print(f\"   Overall Accuracy:   {results['overall_accuracy'][optimal_idx]:.4f}\")\n",
        "    print(f\"   Known Accuracy:     {results['known_accuracy'][optimal_idx]:.4f}\")\n",
        "    print(f\"   Unknown Recall:     {results['unknown_recall'][optimal_idx]:.4f}\")\n",
        "    print(f\"   Weighted F1:        {results['f1_weighted'][optimal_idx]:.4f}\")\n",
        "\n",
        "    # Plot threshold analysis\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    axes[0, 0].plot(results['threshold'], results['overall_accuracy'], 'b-o', linewidth=2)\n",
        "    axes[0, 0].axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.2f}')\n",
        "    axes[0, 0].set_xlabel('Threshold', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Overall Accuracy', fontweight='bold')\n",
        "    axes[0, 0].set_title('Overall Accuracy vs Threshold', fontweight='bold')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    axes[0, 1].plot(results['threshold'], results['known_accuracy'], 'g-o', linewidth=2)\n",
        "    axes[0, 1].axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.2f}')\n",
        "    axes[0, 1].set_xlabel('Threshold', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('Known Classes Accuracy', fontweight='bold')\n",
        "    axes[0, 1].set_title('Known Classes Accuracy vs Threshold', fontweight='bold')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    axes[1, 0].plot(results['threshold'], results['unknown_recall'], 'orange', marker='o', linewidth=2)\n",
        "    axes[1, 0].axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.2f}')\n",
        "    axes[1, 0].set_xlabel('Threshold', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Unknown Class Recall', fontweight='bold')\n",
        "    axes[1, 0].set_title('Unknown Class Recall vs Threshold', fontweight='bold')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    axes[1, 1].plot(results['threshold'], results['f1_weighted'], 'purple', marker='o', linewidth=2)\n",
        "    axes[1, 1].axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.2f}')\n",
        "    axes[1, 1].set_xlabel('Threshold', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Weighted F1 Score', fontweight='bold')\n",
        "    axes[1, 1].set_title('Weighted F1 Score vs Threshold', fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'threshold_analysis.png'), dpi=150)\n",
        "    print(f\"\\n[SAVED] Saved threshold analysis to: results/threshold_analysis.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return optimal_threshold, results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, X, y, threshold, dataset_name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"{dataset_name.upper()} SET EVALUATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions, confidences = predict_with_rejection(model, X, threshold)\n",
        "\n",
        "    # Overall metrics\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "\n",
        "    print(f\"\\n[RESULTS] Overall Performance:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # Per-class metrics\n",
        "    print(f\"\\n[REPORT] Detailed Classification Report:\")\n",
        "    # Provide all possible labels (0-6) and their names to classification_report\n",
        "    all_possible_labels = np.arange(len(CLASS_NAMES))\n",
        "    print(classification_report(y, predictions, labels=all_possible_labels, target_names=CLASS_NAMES, digits=4))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y, predictions, labels=all_possible_labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.xlabel('Predicted', fontweight='bold', fontsize=12)\n",
        "    plt.ylabel('True', fontweight='bold', fontsize=12)\n",
        "    plt.title(f'Confusion Matrix - {dataset_name} Set\\nAccuracy: {accuracy:.4f}',\n",
        "              fontweight='bold', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f'confusion_matrix_{dataset_name.lower()}.png'), dpi=150)\n",
        "    print(f\"\\n[SAVED] Saved confusion matrix to: results/confusion_matrix_{dataset_name.lower()}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Confidence distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for class_id, class_name in enumerate(CLASS_NAMES):\n",
        "        mask = y == class_id\n",
        "        if mask.any():\n",
        "            plt.hist(confidences[mask], bins=30, alpha=0.5, label=class_name)\n",
        "\n",
        "    plt.axvline(threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.2f}')\n",
        "    plt.xlabel('Confidence Score', fontweight='bold')\n",
        "    plt.ylabel('Frequency', fontweight='bold')\n",
        "    plt.title(f'Confidence Distribution - {dataset_name} Set', fontweight='bold', fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f'confidence_distribution_{dataset_name.lower()}.png'), dpi=150)\n",
        "    print(f\"[SAVED] Saved confidence distribution to: results/confidence_distribution_{dataset_name.lower()}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return accuracy, predictions, confidences\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL SAVING\n",
        "# ============================================================================\n",
        "\n",
        "def save_svm_model(model, threshold, best_params, results_summary):\n",
        "    \"\"\"\n",
        "    Save trained SVM model and metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"SAVING SVM MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join(MODELS_DIR, 'svm_model.pkl')\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"\\n[SAVED] Model saved to: {model_path}\")\n",
        "\n",
        "    # Save model configuration\n",
        "    config = {\n",
        "        'best_params': best_params,\n",
        "        'optimal_threshold': threshold,\n",
        "        'results': results_summary,\n",
        "        'class_names': CLASS_NAMES\n",
        "    }\n",
        "\n",
        "    config_path = os.path.join(MODELS_DIR, 'svm_config.json')\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    print(f\"[SAVED] Configuration saved to: {config_path}\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main(quick_mode=False):\n",
        "    \"\"\"\n",
        "    Main training pipeline\n",
        "\n",
        "    Args:\n",
        "        quick_mode: If True, use faster but less thorough hyperparameter tuning\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"[ML] \" * 25)\n",
        "    print(\"PHASE 3: SVM TRAINING & OPTIMIZATION\")\n",
        "    print(\"[ML] \" * 25 + \"\\n\")\n",
        "\n",
        "    # Step 1: Load data\n",
        "    X_train, y_train, X_val, y_val= load_processed_data()\n",
        "\n",
        "    if X_train is None:\n",
        "        return\n",
        "\n",
        "    # Step 2: Hyperparameter tuning\n",
        "    # print(\"\\n\" + \"[TARGET] \" * 35)\n",
        "    # best_params, grid_search = tune_svm_hyperparameters(X_train, y_train, quick_mode=quick_mode)\n",
        "    # analyze_hyperparameter_impact(grid_search)\n",
        "    best_params = {\n",
        "        \"C\": 10,\n",
        "        \"gamma\": \"auto\",\n",
        "        \"kernel\": \"rbf\"\n",
        "    }\n",
        "\n",
        "    # Step 3: Train final model\n",
        "    svm_model = train_final_svm(X_train, y_train, best_params)\n",
        "\n",
        "    # Step 4: Find optimal threshold\n",
        "    optimal_threshold, threshold_results = find_optimal_threshold(svm_model, X_val, y_val)\n",
        "\n",
        "    # Step 5: Evaluate on validation set\n",
        "    val_acc, val_preds, val_confs = evaluate_model(\n",
        "        svm_model, X_val, y_val, optimal_threshold, \"Validation\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Step 7: Evaluate on training set (to check overfitting)\n",
        "    train_acc, _, _ = evaluate_model(\n",
        "        svm_model, X_train, y_train, optimal_threshold, \"Training\"\n",
        "    )\n",
        "\n",
        "    # Step 8: Save model\n",
        "    results_summary = {\n",
        "        'train_accuracy': float(train_acc),\n",
        "        'val_accuracy': float(val_acc),\n",
        "        'feature_dimension': X_train.shape[1],\n",
        "        'training_samples': len(X_train)\n",
        "    }\n",
        "\n",
        "    save_svm_model(svm_model, optimal_threshold, best_params, results_summary)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PHASE 3 COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n[RESULTS] Final Results:\")\n",
        "    print(f\"   Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "    print(f\"   Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "\n",
        "    if train_acc - val_acc > 0.1:\n",
        "        print(f\"\\n[WARNING] Warning: Possible overfitting detected!\")\n",
        "        print(f\"   Train-Test gap: {(train_acc - val_acc)*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"\\n[OK] Good generalization!\")\n",
        "        print(f\"   Train-Test gap: {(train_acc - val_acc)*100:.2f}%\")\n",
        "\n",
        "    if val_acc >= 0.85:\n",
        "        print(f\"\\n[SUCCESS] SUCCESS! Test accuracy >= 85% target!\")\n",
        "    else:\n",
        "        print(f\"\\n[WARNING] Test accuracy below 85% target.\")\n",
        "        print(f\"   Consider: More data augmentation, feature tuning, or ensemble methods\")\n",
        "\n",
        "    print(f\"\\n[DIR] All results saved to: {RESULTS_DIR}/\")\n",
        "    print(f\"[DIR] Model saved to: {MODELS_DIR}/\")\n",
        "\n",
        "    print(f\"\\n[NEXT] Next Steps:\")\n",
        "    print(\"   1. Review confusion matrix to identify problem classes\")\n",
        "    print(\"   2. Analyze misclassified examples\")\n",
        "    print(\"   3. Proceed to Phase 4: Real-time Deployment\")\n",
        "\n",
        "    print(\"\\n\" + \"[ML] \" * 25 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set quick_mode=True for faster testing (5-10 min)\n",
        "    # Set quick_mode=False for best results (15-30 min)\n",
        "    main(quick_mode=False)\n"
      ],
      "metadata": {
        "id": "QyUKvzePA3OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 7: KNN Threshold Calculator (The Helper Script)**\n",
        "\n",
        "* **What it does:** Calculates the average distance of valid images."
      ],
      "metadata": {
        "id": "OFkLWX3jggNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# CONFIGURATION\n",
        "MODEL_DIR = '/content/drive/MyDrive/CNN/saved_models'\n",
        "DATA_DIR = '/content/drive/MyDrive/CNN/features'\n",
        "KNN_MODEL_PATH = os.path.join(MODEL_DIR, 'knn_model.pkl')\n",
        "\n",
        "def calculate_threshold():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"KNN DISTANCE THRESHOLD CALCULATOR\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Load Data and Model\n",
        "    print(f\"[INFO] Loading data from {DATA_DIR}...\")\n",
        "    try:\n",
        "        X_train = np.load(os.path.join(DATA_DIR, 'X_train.npy'))\n",
        "        X_val = np.load(os.path.join(DATA_DIR, 'X_val.npy'))\n",
        "        y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))\n",
        "\n",
        "        print(f\"[INFO] Loading KNN model from {KNN_MODEL_PATH}...\")\n",
        "        with open(KNN_MODEL_PATH, 'rb') as f:\n",
        "            knn = pickle.load(f)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"[ERROR] {e}\")\n",
        "        print(\"Please ensure you have run the training phases first.\")\n",
        "        return\n",
        "\n",
        "    # 2. Calculate Distances for Validation Data\n",
        "    # We use validation data because it wasn't used to build the tree,\n",
        "    # so it simulates 'new' data better than training data.\n",
        "    print(\"[INFO] Calculating distances for validation set...\")\n",
        "\n",
        "    # kneighbors returns (distances, indices)\n",
        "    # distances shape: (num_samples, n_neighbors)\n",
        "    distances, _ = knn.kneighbors(X_val)\n",
        "\n",
        "    # We care about the average distance to the k neighbors\n",
        "    mean_distances = distances.mean(axis=1)\n",
        "\n",
        "    # 3. Analyze the Statistics\n",
        "    min_dist = np.min(mean_distances)\n",
        "    max_dist = np.max(mean_distances)\n",
        "    avg_dist = np.mean(mean_distances)\n",
        "    std_dist = np.std(mean_distances)\n",
        "\n",
        "    # Calculate percentiles (safe zones)\n",
        "    p95 = np.percentile(mean_distances, 95)\n",
        "    p99 = np.percentile(mean_distances, 99)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"DISTANCE STATISTICS (KNOWN CLASSES)\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Min Distance:     {min_dist:.4f}\")\n",
        "    print(f\"Average Distance: {avg_dist:.4f}\")\n",
        "    print(f\"Max Distance:     {max_dist:.4f}\")\n",
        "    print(f\"Std Deviation:    {std_dist:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"95% of data is below: {p95:.4f}\")\n",
        "    print(f\"99% of data is below: {p99:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # 4. Recommendation Logic\n",
        "    # A safe threshold is usually Mean + 3*StdDev or the 99th percentile.\n",
        "    # We want to accept 99% of valid images, and reject anything further out.\n",
        "    recommended_threshold = p99 * 1.1  # Add 10% buffer\n",
        "\n",
        "    print(f\"\\n[RECOMMENDATION] Suggested Threshold: {recommended_threshold:.2f}\")\n",
        "    print(f\"(Use this value for 'dist_threshold' in your predict function)\")\n",
        "\n",
        "    # 5. Visual Histogram\n",
        "    # This chart helps you verify if the threshold makes sense visually\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(mean_distances, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    plt.axvline(p95, color='orange', linestyle='--', linewidth=2, label=f'95th Percentile ({p95:.2f})')\n",
        "    plt.axvline(recommended_threshold, color='red', linestyle='--', linewidth=2, label=f'Recommended ({recommended_threshold:.2f})')\n",
        "\n",
        "    plt.title('KNN Distance Distribution (Validation Set)')\n",
        "    plt.xlabel('Average Distance to Neighbors')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(MODEL_DIR, 'knn_distance_distribution.png')\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"\\n[GRAPH] Saved distribution plot to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    calculate_threshold()"
      ],
      "metadata": {
        "id": "PBAITX2GBFPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2ce0060-b06d-4841-953b-57ef744eb2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "KNN DISTANCE THRESHOLD CALCULATOR\n",
            "============================================================\n",
            "[INFO] Loading data from /content/drive/MyDrive/CNN/features...\n",
            "[INFO] Loading KNN model from /content/drive/MyDrive/CNN/saved_models/knn_model.pkl...\n",
            "[INFO] Calculating distances for validation set...\n",
            "\n",
            "----------------------------------------\n",
            "DISTANCE STATISTICS (KNOWN CLASSES)\n",
            "----------------------------------------\n",
            "Min Distance:     20.5660\n",
            "Average Distance: 45.5757\n",
            "Max Distance:     69.5929\n",
            "Std Deviation:    8.1392\n",
            "----------------------------------------\n",
            "95% of data is below: 59.0312\n",
            "99% of data is below: 64.4935\n",
            "----------------------------------------\n",
            "\n",
            "[RECOMMENDATION] Suggested Threshold: 70.94\n",
            "(Use this value for 'dist_threshold' in your predict function)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/content/drive/MyDrive/CNN/saved_models/knn_distance_distribution.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3283484561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mcalculate_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3283484561.py\u001b[0m in \u001b[0;36mcalculate_threshold\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'knn_distance_distribution.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[GRAPH] Saved distribution plot to {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content/drive/MyDrive/CNN/saved_models/knn_distance_distribution.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAIjCAYAAADWYVDIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAf69JREFUeJzt3Xd8VFX6x/HvpA+kACmkEELoRemIgHSkiK4UC4osCILwAwuorNgAccWCiAVBXQVFEBcLdhQQELEBLiAWSoQAoYWSTALpc35/sJmdIYUUkknI5/16DWHuPeeeZ+6cmdwn99xzLcYYIwAAAACAJMnD3QEAAAAAQEVCkgQAAAAATkiSAAAAAMAJSRIAAAAAOCFJAgAAAAAnJEkAAAAA4IQkCQAAAACckCQBAAAAgBOSJAAAAABwQpIEAE4WL14si8Wi/fv3uzuUSqU899uoUaNUr149x/P9+/fLYrFozpw5Zd62JM2YMUMWi6Vc2srPwYMH5efnp02bNpVZG/m9nz169FCPHj0uWHf9+vWyWCxav379RY3JYrFoxowZF3WbFc2qVavk7++vxMREd4cCVHkkSQBKJPcgasuWLS7Lk5OTdcUVV8jPz0+rVq2S9L+Dytq1a+vs2bN5tlWvXj1de+21LsssFossFouee+65Ird9vtyDtdyHr6+vateurR49eujJJ5+8aAciZ8+e1YwZMy76QaG7sN8qdmyPP/64OnbsqC5duigrK0shISG66qqrCixvjFF0dLTatm1bjlGWzBdffFEhE6HvvvtOAwYMUFRUlPz8/FS3bl1dd911WrZsWYm298orr2jx4sV5lvfv318NGzbU7NmzSxkxgNIiSQJw0dhsNvXt21c7duzQRx99pP79+7usP378uBYsWFCsbT777LP5JlbFcffdd2vJkiV67bXX9MADD6hWrVqaPn26mjVrpm+++cal7IgRI5SWlqaYmJgib//s2bOaOXNmhTygLo2Kut9ef/117dq1q1h1iquw2B555BGlpaWVafsFSUxM1FtvvaXx48dLkry9vXXjjTfq+++/V3x8fL51vv32Wx06dEi33XZbqdr++uuv9fXXX5dqGxfyxRdfaObMmfmuS0tL0yOPPFKm7ednxYoV6tatm44dO6Z77rlHL730km677TadPn1ar7/+eom2WVCSJEl33nmnXn31VaWkpJQiagCl5eXuAABcGlJSUtSvXz9t27ZNH374oQYMGJCnTOvWrfXss8/q//7v/2S1Wi+4zdatW2vbtm1auHChpkyZUuLYunbtqhtuuMFl2fbt29W3b18NHTpUv//+uyIiIiRJnp6e8vT0LHFbl5KKtt/OnDmj6tWry9vbu0zbuRAvLy95ebnn1+c777wjLy8vXXfddY5lw4cP18KFC/Xuu+/qwQcfzFNn2bJl8vDw0LBhw0rVto+PT6nql5afn59b2p0xY4aaN2+uH3/8Mc8+OH78+EVvb+jQobrrrru0YsUKjR49+qJvH0DRcCYJQKmlpqaqf//++uWXX/TBBx9o4MCB+ZZ77LHHdOzYsSKfTerSpYt69eqlZ5555qL/5b5Vq1aaN2+ekpKS9PLLLzuW53ctxpYtW9SvXz+FhITIarUqNjbWcfCyf/9+hYaGSpJmzpzpGKKWO2Rox44dGjVqlOrXry8/Pz+Fh4dr9OjROnnypEs8uUMS9+7dq1GjRqlGjRoKCgrS7bffnu+ZtHfeeUdXXHGFqlWrppo1a6pbt255/sr/5ZdfqmvXrqpevboCAgI0cOBA/fbbb5Viv40aNUr+/v6Ki4vTNddco4CAAA0fPtyxzvmaJGfPP/+8YmJiZLVa1b17d+3cudNlfUHX1Thv80Kx5XdNUnZ2tmbNmqUGDRrI19dX9erV00MPPaSMjAyXcrlDS7/77jvHsNT69evr7bffzn+Hn2flypXq2LGj/P39Hcu6dOmievXq5Tv0KysrS++//7569uypyMjIIvfH/OS37w4dOqRBgwapevXqCgsL0+TJk/O8ZknauHGjbrzxRtWtW1e+vr6Kjo7W5MmTXT7Xo0aN0vz58yXJZbhnrvyuSfrPf/6jAQMGKDAwUP7+/urdu7d+/PFHlzK5fXPTpk2aMmWKQkNDVb16dQ0ePLhIQ0fj4uLUoUOHfJPEsLAwl+d2u13z5s1TixYt5Ofnp9q1a+vOO+/U6dOnHWXq1aun3377TRs2bHC8Ruf9GhYWppYtW+rjjz++YGwAyg5JEoBSOXPmjAYMGKDNmzdrxYoVea4tcta1a9diJz0zZswoVmJVHDfccIOsVmuhQ4iOHz+uvn37av/+/XrwwQf10ksvafjw4Y4DsdDQUEdsgwcP1pIlS7RkyRINGTJEkrR69Wr99ddfuv322/XSSy9p2LBhWr58ua655hoZY/K0d9NNNyklJUWzZ8/WTTfdpMWLF+cZfjRz5kyNGDFC3t7eevzxxzVz5kxFR0e7DIFbsmSJBg4cKH9/fz399NN69NFH9fvvv+uqq64q9eQK5bHfpHOJR79+/RQWFqY5c+Zo6NChhcb19ttv68UXX9TEiRM1bdo07dy5U7169dKxY8eK9fqKEtv57rjjDj322GNq27atnn/+eXXv3l2zZ8/O9+zN3r17dcMNN+jqq6/Wc889p5o1a2rUqFEXTGCzsrK0efPmPNcWWSwW3Xrrrfr111/zbGPVqlU6deqUI8Esbn8sTFpamnr37q2vvvpKkyZN0sMPP6yNGzdq6tSpecquWLFCZ8+e1YQJE/TSSy+pX79+eumll/T3v//dUebOO+/U1VdfLUmOfb5kyZIC2//tt9/UtWtXbd++XVOnTtWjjz6qffv2qUePHvrpp5/ylL/rrru0fft2TZ8+XRMmTNCnn36qSZMmXfB1xsTEaO3atTp06NAFy95555164IEH1KVLF73wwgu6/fbbtXTpUvXr109ZWVmSpHnz5qlOnTpq2rSp4zU+/PDDLttp166dvv/++wu2B6AMGQAogUWLFhlJJiYmxnh7e5uVK1cWWHb69OlGkklMTDQbNmwwkszcuXMd62NiYszAgQNd6kgyEydONMYY07NnTxMeHm7Onj3r0vbmzZsLjXHdunVGklmxYkWBZVq1amVq1qyZ53Xt27fPGGPMRx99dMG2EhMTjSQzffr0POtyY3b27rvvGknm22+/dSzL3UejR492KTt48GATHBzseL5nzx7j4eFhBg8ebHJyclzK2u12Y4wxKSkppkaNGmbs2LEu648ePWqCgoLyLD9fRdhvI0eONJLMgw8+mO+6mJgYx/N9+/YZScZqtZpDhw45lv/0009Gkpk8ebJjWffu3U337t0vuM3CYst9r3Jt27bNSDJ33HGHS7n777/fSDLffPONY1lMTEye9/748ePG19fX3HfffXnacrZ3714jybz00kt51v32229Gkpk2bZrL8mHDhhk/Pz+TnJxsjCl6fzz//TQm776bN2+ekWT+/e9/O5adOXPGNGzY0Egy69atcyzPr93Zs2cbi8Vi4uPjHcsmTpxoCjo0Of/9GDRokPHx8TFxcXGOZYcPHzYBAQGmW7dueV5Lnz59HJ8RY4yZPHmy8fT0NElJSfm2l+uNN94wkoyPj4/p2bOnefTRR83GjRvzfP42btxoJJmlS5e6LF+1alWe5S1atMi3H+Z68sknjSRz7NixQmMDUHY4kwSgVI4dOyY/Pz9FR0cXqXy3bt3Us2fPYp9NOnr0qBYuXFiaUPPl7+9f6AXSNWrUkCR99tlnjr8EF4fztVfp6ek6ceKErrzySknSL7/8kqd87gX5ubp27aqTJ0/KZrNJOjfcym6367HHHpOHh+tXeO7QpNWrVyspKUm33HKLTpw44Xh4enqqY8eOWrduXbFfx/nKer/lmjBhQpHLDho0SFFRUY7nV1xxhTp27KgvvviixO0XRe72z79u7r777pMkff755y7Lmzdvrq5duzqeh4aGqkmTJvrrr78KbSd3SFzNmjXzrGvevLnatGmj5cuXO5adOXNGn3zyia699loFBgZKKn5/LMwXX3yhiIgIl+vWqlWrpnHjxuUp69zumTNndOLECXXu3FnGGP3nP/8pVruSlJOTo6+//lqDBg1S/fr1HcsjIiJ066236rvvvnN8ZnKNGzfOZfhe165dlZOTU+CEF7lGjx6tVatWqUePHvruu+80a9Ysde3aVY0aNXI527NixQoFBQXp6quvdvnctWvXTv7+/sX63OW+xydOnChyHQAXF0kSgFJ59dVX5ePjo/79+xd5xrHiJj0lSayKKjU1VQEBAQWu7969u4YOHaqZM2cqJCRE119/vRYtWpTvdRf5OXXqlO655x7Vrl1bVqtVoaGhio2NlXRuuvTz1a1b1+V57sFS7jUNcXFx8vDwUPPmzQtsc8+ePZKkXr16KTQ01OXx9ddfX5SLzct6v0nnJkioU6dOkcs3atQoz7LGjRuX+b2b4uPj5eHhoYYNG7osDw8PV40aNfIchJ//Hkvn3mfn61YKYwoYFjd8+HDt27fPceC+cuVKnT171jHUTip+fyxMfHy8GjZsmOf6rCZNmuQpe+DAAY0aNUq1atWSv7+/QkND1b179xK1K52b5e/s2bP5ttWsWTPZ7XYdPHjQZfmFPluF6devn7766islJSXp22+/1cSJExUfH69rr73W8Xnas2ePkpOTFRYWludzl5qaWqzPXe577M77cQFVHbPbASiV5s2b64svvlDv3r119dVXa9OmTRc8q9StWzf16NFDzzzzTJ4zJwWZPn26evTooVdffdVxlqK0srKytHv3bl122WUFlrFYLHr//ff1448/6tNPP9VXX32l0aNH67nnntOPP/7ocgF9fm666SZ9//33euCBB9S6dWv5+/vLbrerf//+stvtecoXNENcQQfG+cnd7pIlSxQeHp5nfWlnZiuP/SZJvr6+ec6WlZbFYsl3X+bk5FyUbRdFSd/j4OBgSQUf1N9yyy2aOnWqli1bps6dO2vZsmWqWbOmrrnmGkeZ4vbHiyEnJ0dXX321Tp06pX/84x9q2rSpqlevroSEBI0aNarM2j3fxfhsVatWTV27dlXXrl0VEhKimTNn6ssvv9TIkSNlt9sVFhampUuX5ls3dzKQosh9j0NCQopcB8DFRZIEoNSuuOIKrVy5UgMHDtTVV1+tjRs3XvCAYMaMGY6kpyi6d++uHj166Omnn9Zjjz12McLW+++/r7S0NPXr1++CZa+88kpdeeWV+uc//6lly5Zp+PDhWr58ue64444CD45Pnz6ttWvXaubMmS4x557pKYkGDRrIbrfr999/V+vWrQssI52bJatPnz4lbqsgZb3fSiq//bp7926XmfBq1qyZ77C288/2FCe2mJgY2e127dmzR82aNXMsP3bsmJKSkop176jC1K1bV1arVfv27ct3fWRkpHr27KkVK1bo0Ucf1erVqzVq1CjHrGwXuz/GxMRo586dMsa47K/zzyj/+uuv2r17t9566y2XiRpWr16dZ5tF3e+hoaGqVq1avmev//zzT3l4eBR5CHBJtW/fXpJ05MgRSec+d2vWrFGXLl0ueIuDC73Offv2KSQkpFiJFYCLi+F2AC6K3r17691339XevXvVv3//PNcDnM856UlPTy9SG7nD9F577bVSx7t9+3bde++9qlmzpiZOnFhgudOnT+f5S3NucpI7dKxatWqSpKSkJJdyuX+5Pr/+vHnzShz3oEGD5OHhoccffzzPX+Bz2+nXr58CAwP15JNP5ns9UFGmPS5Ieey3klq5cqUSEhIcz3/++Wf99NNPLvfsatCggf7880+XfbB9+3Zt2rTJZVvFiS33TM357+vcuXMlqcAp8YvL29tb7du315YtWwosM3z4cB0/flx33nmnsrKyXIbaXez+eM011+jw4cN6//33HcvOnj2b5/OZX7vGGL3wwgt5tlm9enVJF97vnp6e6tu3rz7++GOX4ZTHjh3TsmXLdNVVVzmuwyqttWvX5rs891q03CF/N910k3JycjRr1qw8ZbOzs11eU/Xq1Qt9jVu3blWnTp1KHjSAUuNMEoCLZvDgwXr99dc1evRo/e1vf9OqVasKvQHk9OnT1bNnzyJvv3v37urevbs2bNhQrLg2btyo9PR05eTk6OTJk9q0aZM++eQTBQUF6aOPPsp3SFqut956S6+88ooGDx6sBg0aKCUlRa+//roCAwMdB8dWq1XNmzfXe++9p8aNG6tWrVq67LLLdNlll6lbt2565plnlJWVpaioKH399dcFngkoioYNG+rhhx92XDw+ZMgQ+fr6avPmzYqMjNTs2bMVGBioBQsWaMSIEWrbtq2GDRum0NBQHThwQJ9//rm6dOnico+jirjfSrpvrrrqKk2YMEEZGRmaN2+egoODXaakHj16tObOnat+/fppzJgxOn78uBYuXKgWLVq4JPbFia1Vq1YaOXKkXnvtNSUlJal79+76+eef9dZbb2nQoEHF6uMXcv311+vhhx+WzWbLNwkYOnSo/u///k8ff/yxoqOj1a1bN8e6wMDAi9ofx44dq5dffll///vftXXrVkVERGjJkiWOBDNX06ZN1aBBA91///1KSEhQYGCgPvjgg3yHDbZr106SdPfdd6tfv37y9PQs8Ca4TzzxhFavXq2rrrpK//d//ycvLy+9+uqrysjI0DPPPFOi15Sf66+/XrGxsbruuuvUoEEDnTlzRmvWrNGnn36qDh06OG7s2717d915552aPXu2tm3bpr59+8rb21t79uzRihUr9MILLzgmuWjXrp0WLFigJ554Qg0bNlRYWJh69eol6dz0+Tt27Cj0jxAAyoEbZtQDcAkobBruOXPmGEnm2muvNVlZWS5TgJ+ve/fuRlKhU4A7y52euqC2CyoryXh7e5vQ0FDTrVs3889//tMcP368wNeVO/XxL7/8Ym655RZTt25d4+vra8LCwsy1115rtmzZ4lLv+++/N+3atTM+Pj4uUxUfOnTIDB482NSoUcMEBQWZG2+80Rw+fDjPdMYF7aP8pmI2xpg333zTtGnTxvj6+pqaNWua7t27m9WrV+d5/f369TNBQUHGz8/PNGjQwIwaNSpP7BVxv40cOdJUr1493/gKmgL82WefNc8995yJjo42vr6+pmvXrmb79u156r/zzjumfv36xsfHx7Ru3dp89dVXebZZWGznTwFujDFZWVlm5syZJjY21nh7e5vo6Ggzbdo0k56e7lIuv+nujSl4avLzHTt2zHh5eZklS5YUWObGG280kszUqVPzrCtqfyzKFODGGBMfH2/+9re/mWrVqpmQkBBzzz33OKa8dp4C/Pfffzd9+vQx/v7+JiQkxIwdO9Zs377dSDKLFi1ylMvOzjZ33XWXCQ0NNRaLxWU/nx+jMef6Wb9+/Yy/v7+pVq2a6dmzp/n+++9dyhT0XZXbz53jzM+7775rhg0bZho0aGCsVqvx8/MzzZs3Nw8//LCx2Wx5yr/22mumXbt2xmq1moCAAHP55ZebqVOnmsOHDzvKHD161AwcONAEBAQYSS77dcGCBaZatWr5bhtA+bEYU8y7xwEAALcZM2aMdu/erY0bN7o7FJSBNm3aqEePHnr++efdHQpQpZEkAQBQiRw4cECNGzfW2rVr1aVLF3eHg4to1apVuuGGG/TXX38pLCzM3eEAVRpJEgAAAAA4YXY7AAAAAHBCkgQAAAAATkiSAAAAAMAJSRIAAAAAOLnkbyZrt9t1+PBhBQQEyGKxuDscAAAAAG5ijFFKSooiIyPl4VHw+aJLPkk6fPiwoqOj3R0GAAAAgAri4MGDqlOnToHrL/kkKSAgQNK5HREYGFimbdntdiUmJio0NLTQzBTID/0HJUXfQWnQfyBJ+rSplHZEskZI1/1Z5Gr0H5RGifpP06bSkSNSRIT0Z9H7ai6bzabo6GhHjlCQSz5Jyh1iFxgYWC5JUnp6ugIDA/miQLHRf1BS9B2UBv0HkqRqHpJFktVDKsbxEv0HpVGi/pNbzqN4ffV8F7oMh94MAAAAAE5IkgAAAADACUkSAAAAADi55K9JKgpjjLKzs5WTk1Oq7djtdmVlZSk9PZ1xuSi2yth/vL295enp6e4wAACl1X+zZHIkC9/pqOA2b5ZycqQyPv6o8klSZmamjhw5orNnz5Z6W8YY2e12paSkcE8mFFtl7D8Wi0V16tSRv7+/u0MBAJSGNcLdEQBFE1E+fbVKJ0l2u1379u2Tp6enIiMj5ePjU6qD09wzUl5eXpXmIBcVR2XrP8YYJSYm6tChQ2rUqBFnlAAAwCWjSidJmZmZstvtio6OVrVq1Uq9vcp2kIuKpTL2n9DQUO3fv19ZWVkkSQAA4JJRpZOkXJXl+g+goqksyRwA4AL2viZlpUre/lLDce6OBijYa69JqamSv780ruz6KkkSAABAVffr41JagmSNIklCxfb441JCghQVVaZJEqdQAAAAAMAJSRJcjBo1SoMGDXJ3GG61fv16WSwWJSUlSZIWL16sGjVqlHq7mZmZatiwob7//vtSb6s8PPjgg7rrrrvcHQYAAEC5I0mqhFJSUnTvvfcqJiZGVqtVnTt31ubNm13KjBo1ShaLxeXRv39/x/r9+/fLYrFo27ZtpY5n8eLFjjY8PDxUp04d3X777Tp+/Hipt13WevTooXvvvddlWefOnXXkyBEFBQVd1LYWLlyo2NhYde7c2bGsXr16LvvOx8dHTz31lEu9f//732rdurWqVaummJgYPfvssxds69SpUxo+fLgCAwNVo0YNjRkzRqmpqY71u3btUs+ePVW7dm35+fmpfv36euSRR5SVleUoc//99+utt97SX3/9dRFePQAAQOXBNUmV0B133KGdO3dqyZIlioyM1DvvvKM+ffro999/V1RUlKNc//79tWjRIsdzX1/fMospMDBQu3btkt1u1/bt23X77bfr8OHD+uqrr0q0vaysLHl7e1/kKIvGx8dH4eHhF3Wbxhi9/PLLevzxx/Ose/zxxzV27FjH7HY1a9Z0rPvyyy81fPhwvfTSS+rbt6/++OMPjR07VlarVZMmTSqwveHDh+vIkSNavXq1srKydPvtt2vcuHFatmyZpHM3gf373/+utm3bqkaNGtq+fbvGjh0ru92uJ598UpIUEhKifv36acGCBUVKzAAAAC4VnEmqZNLS0vTBBx/omWeeUbdu3dSwYUPNmDFDDRs21IIFC1zK+vr6Kjw83PFwPviOjY2VJLVp00YWi0U9evRwqTtnzhxFREQoODhYEydOdDnDkB+LxaLw8HBFRkZqwIABuvvuu7VmzRqlpaVJkv71r3+pWbNm8vPzU9OmTfXKK6846uae1XrvvffUvXt3+fn5aenSpZKkN998Uy1atJCvr68iIiJcEoOkpCTdcccdCg0NVWBgoHr16qXt27c71s+YMUOtW7fWkiVLVK9ePQUFBWnYsGFKSUmRdO5s24YNG/TCCy84zubs378/z3C7/Hz88cdq27at4yzMzJkzlZ2dXWD5rVu3Ki4uTgMHDsyzLiAgwOV9ql69umPdkiVLNGjQII0fP17169fXwIEDNW3aND399NMyxuTb1h9//KFVq1bpX//6lzp27KirrrpKL730kpYvX67Dhw9LkurXr6/bb79drVq1UkxMjP72t79p+PDh2rhxo8u2rrvuOi1fvrzA1wUAAHAp4kxSfv6YK/0598LlarWVun/issjzu8FS0rYL1206RWo2pdihZWdnKycnR35+fi7LrVarvvvuO5dl69evV1hYmGrWrKlevXrpiSeeUHBwsCTp559/1hVXXKE1a9aoRYsW8vHxcdRbt26dIiIitG7dOu3du1c333yzWrdurbFjxxY5TqvVKrvdruzsbC1dulSPPfaYXn75ZbVp00b/+c9/NHbsWFWvXl0jR4501HnwwQf13HPPqU2bNvLz89OCBQs0ZcoUPfXUUxowYICSk5O1adMmR/kbb7xRVqtVX375pYKCgvTqq6+qd+/e2r17t2rVqiVJiouL08qVK/XZZ5/p9OnTuummm/TUU0/pn//8p1544QXt3r1bl112meMMT+59fwqzceNG/f3vf9eLL76orl27Ki4uTuP+O7vK9OnTC6zTuHFjBQQE5Fn31FNPadasWapbt65uvvlm3XfffY6zaBkZGXnu4WW1WnXo0CHFx8erXr16ebb3ww8/qEaNGmrfvr1jWZ8+feTh4aGffvpJgwcPzlNn7969WrVqlYYMGeKy/IorrtChQ4e0f//+fNsCAAC4FLn1TNLs2bPVoUMHBQQEKCwsTIMGDdKuXbtcyvTo0SPPtTXjx48v28CybOemwbzQIz0xb92ME7IUpW6WrUShBQQEqFOnTpo1a5YOHz6snJwcvfPOO/rhhx905MgRR7n+/fvr7bff1tq1a/X0009rw4YNGjBggHJyciSdSwYkKTg4WOHh4Y6kQpJq1qypl19+WU2bNtW1116rgQMHau3atUWOcc+ePVq4cKHat2+vgIAATZ8+Xc8995yGDBmi2NhYDRkyRJMnT9arr77qUu/ee+91lImIiNATTzyh++67T/fcc48aN26sDh06OK4f+u677/Tzzz9rxYoVat++vRo1aqQ5c+aoRo0aev/99x3btNvtWrx4sS677DJ17dpVI0aMcLyWoKAg+fj4qFq1ao6zOEW5IerMmTP14IMPauTIkapfv76uvvpqzZo1K8/rcRYfH6/IyMg8y++++24tX75c69at07hx4/T0009r6tSpjvX9+vXThx9+qLVr18put2v37t167rnnJMnl/XZ29OhRhYWFuSzz8vJSrVq1dPToUZflnTt3lp+fnxo1aqSuXbvmGQ6YG3N8fHwhewQAAODS4tYzSRs2bNDEiRPVoUMHZWdn66GHHlLfvn31+++/uww5Gjt2rMvB2/l/Wb/ovAPP3SfgQvxC8y7zDZGxRumCt9j0DixJZJLODcEaPXq0oqKi5OnpqbZt2+qWW27R1q1bHWWGDRvm+P/ll1+uli1bqkGDBlq/fr169+5d6PZbtGjhkixERETo119/LbROcnKy/P39ZbfblZ6erquuukr/+te/dObMGcXFxWnMmDEuZ6Kys7PzTIzgfObj+PHjOnz4cIGxbt++XampqY4zY7nS0tIUFxfneF6vXj2XszcRERGlnlBi+/bt2rRpk/75z386luXk5Cg9PV1nz57Nt3+mpaXlOfsnSVOm/O9s4uWXXy4vLy/93//9n5566in5+vpq7NixiouL07XXXqusrCwFBgbqnnvu0YwZMy7KTZDfe+89paSkaPv27XrggQc0Z84clyTNarVKks6ePVvqtgAAACoLtyZJq1atcnm+ePFihYWFaevWrerWrZtjee5f+stNs5INhZOknKs+kpeXl2S5YJpUYg0aNNCGDRt05swZ2Ww2RURE6Oabb1b9+vULrFO/fn2FhIRo7969F0ySzp8wwWKxyG63F1onICBAv/zyizw8PBQREeE4uD527Jgk6fXXX1fHjh1d6px/1sY5Mc6tX5DU1FRFRERo/fr1edY5T9ddktdyIampqZo5c2aeoWmS8k2EpHOTIFwo0ZTk+IPB/v371aRJE1ksFj399NN68skndfToUYWGhjrOhBX0foeHh+dJBLOzs3Xq1Kk8n6Po6GhJUvPmzZWTk6Nx48bpvvvuc7w3p06dkvS/M48AgEtUYGPJJ0jyq+3uSIDCNW4sBQVJtcu2r1aoa5KSk5MlyWXolyQtXbpU77zzjsLDw3Xdddfp0UcfLfBsUkZGhjIyMhzPbbZzw9rsdnueg2O73S5jjONxMeRu52JtrzDVqlVTtWrVdOrUKX311VeFXsx/6NAhnTx5UuHh4TLGOJKH7OzsfOs4L7vQazLGyMPDQw0aNMhTJywsTJGRkYqLi9Ott96ab13n7ef+39/fX/Xq1dOaNWvyTCohnZtw4ujRo/L09Mz3Wpnzt1vQa/Hx8cmzD86P5/w6bdu21Z9//unyes9v+3ytW7fWggULZLfbZSkkgd6+fbs8PDwUGhrqsh0PDw/H0Ld3331XnTp1UkhISL5tXXnllUpKStKWLVvUrl07SXIM17viiisKfB9zcnKUlZWlnJwcx1mqX3/9Vd7e3mrevHmB/cQYk+/nC+Un97uM96BsnDhxwvG7pLgCAwMVEhJykSO6uOg/kCT1XPO//xejL9B/UBol6j9rStZXndssigqTJNntdt17773q0qWLLrvsMsfyW2+9VTExMYqMjNSOHTv0j3/8Q7t27dKHH36Y73Zmz56tmTNn5lmemJio9PR0l2VZWVmOyQUKm5msqIwxjmt+CjsQLq2vv/5axhg1btxYcXFxevDBB9WkSRONGDFC2dnZSk1N1RNPPKHBgwerdu3a+uuvvzRt2jQ1aNBAvXv3VnZ2tmrVqiWr1aovvvhC4eHh8vPzU1BQkONg13l/5B4IF7SPcjtbQesfe+wxTZ48WQEBAerbt68yMjL0yy+/6PTp07r33nsd9c5/Hx555BFNmjTJMRV1amqqvv/+e02cOFE9evTQlVdeqUGDBmn27Nlq1KiRjhw5oi+++EKDBg1Su3btHB88522eH2vdunX1008/ae/evfL391etWrUc72FuPOfXeeihhzRo0CDVqVNHQ4YMkYeHh3bs2KHffvst3ym+Jalr165KTU3V9u3bHf37xx9/1M8//6zu3bsrICBAP/74ox544AHdeuutCggIUHZ2tk6cOKEPP/xQ3bp1U3p6ut5++22tWLFCa9eudcSzefNm3X777frqq68UFRWlRo0aqV+/fho7dqzmz5+vrKwsTZo0STfddJPCwsKUnZ2tZcuWydvbW5dddpl8fX21detWPfTQQ7rxxhtlsVgc296wYYOuuuoqeXt75/v+5u6fkydPum3Kdpzr18nJyY4/WODiSU5O1osvv6K0zMwS1bf6+OjuSf930e+7djHRf1Aa9B+Uhjv6T+4sxxdSYZKkiRMnaufOnXlmaMudNUw6d81GRESEevfurbi4uHz/kj9t2jSX6zxsNpuio6Md00Q7S09PV0pKiry8vM4NkbtIyvpgMTU1VQ899JAOHTqkWrVqaciQIfrnP//pGKLm6+vruI9SUlKSIiMjHZML5A5p8/Ly0gsvvKBZs2Zp5syZ6tq1q9atWycPDw95eHi47I/cCTMK2ke5nbqg9ePGjZO/v7/mzJmjBx98UNWrV9fll1+ue+65x2Xfn/8+jB49WllZWZo3b57+8Y9/KCQkREOHDnWU+eKLL/Twww9r7NixSkxMVHh4uLp166bIyEh5eXnJw8MjT9znx/rAAw9o1KhRatWqldLS0vTXX385hprlxnN+nWuuuUaffvqpZs2apTlz5sjb21tNmzbVmDFjCtwHtWvX1uDBg/Xee++pdevWks6dCVyxYoVmzZqljIwMxcbG6u6779b999/vsk/eeecd/eMf/5AxRp06ddK6detchi5mZGRo9+7dMsY46i1dulR33XWX+vXrJw8PDw0ZMkQvvviiY72vr6+effZZR72YmBhNnDhRkydPdnkNK1as0PTp0wt8Xbn7Jzg4uMChhih7uWcoQ0NDOUi5yFJTU/X73jh1v+1OBYfXKVbdk0cPacM7r8rT0zPPZCoVCf0HpUH/QWm4o/8U9XjFYspjXNgFTJo0SR9//LG+/fZbx/17CnLmzBn5+/tr1apV6tev3wW3bbPZFBQUpOTk5HyTpH379ik2NvaiHODlnrXw8vIq0zNJqJx27Nihq6++WnFxcfL398+zvqL1ny+//FL33XefduzYUWCSdLE/QygZu92u48ePKywsjIOUiywuLk5jJt6toVP/qfCYwn8/ne9o/D598MzDemP+iwUOz60I6D8oDfoPSsMd/aew3MCZW88kGWN011136aOPPtL69esvmCBJ0rZt2ySdm6UMqExatmypp59+Wvv27dPll1/u7nAu6MyZM1q0aNFFPcsKAKigNg2XMk5IviFSl6XujgYo2PDh0okTUkiItLTs+qpbj34mTpyoZcuW6eOPP1ZAQIDjHi5BQUGyWq2Ki4vTsmXLdM011yg4OFg7duzQ5MmT1a1bN7Vs2dKdoQMlMmrUKHeHUGQ33HCDu0MAAJSX4xvO3cexKLdAAdxpwwYpIUGKKtu+6tYkacGCBZKUZ/ayRYsWadSoUfLx8dGaNWs0b948nTlzRtHR0Ro6dKgeeeQRN0QLAAAAoCpw+3C7wkRHR2vDhg3lFA0AAAAASFxhBwAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACckSQAAAADghCQJVVq9evU0b968Um1jxowZat269QXLPfrooxo3blyp2iovv//+u+rUqaMzZ864OxQAAIByR5JUCY0aNUoWi0UWi0Xe3t6KjY3V1KlTlZ6e7u7QUICjR4/qhRde0MMPP+xYVq9ePcf7aLFY5OHhIR8fH02cONFRJj09XRMnTlRwcLD8/f01dOhQHTt2rNC2jh07plGjRikyMlLVqlVT//79tWfPnnzLGmM0YMAAWSwWrVy50rG8efPmuvLKKzV37tzSvXAAQOXQcKzUZPK5n0BFNnasNHnyuZ9lyK33SULJ9e/fX4sWLVJWVpa2bt2qkSNHymKx6Omnn3Z3aMjHv/71L3Xu3FkxMTGOZZs3b1ZOTo7j+a+//qq+ffvqxhtvdCybPHmyPv/8c61YsUJBQUGaNGmShgwZok2bNuXbjjFGgwYNkre3tz7++GMFBgZq7ty56tOnj37//XdVr17dpfy8efNksVjy3dbtt9+usWPHatq0afLy4qsCAC5pl093dwRA0Uwvn77KmaRKytfXV+Hh4YqOjtagQYPUp08frV692rHebrdr9uzZio2NldVqVatWrfT++++7bOO3337Ttddeq8DAQAUEBKhr166Ki4tz1H/88cdVp04d+fr6qnXr1lq1apWj7v79+2WxWPTvf/9bXbt2ldVqVYcOHbR7925t3rxZ7du3l7+/vwYMGKDExERHvVGjRmnQoEF68sknVbt2bdWoUUOPP/64srOz9cADD6hWrVqqU6eOFi1a5BLrwYMHddNNN6lGjRqqVauWrr/+eu3fvz/PdufMmaOIiAgFBwdr4sSJysrKcpQ5fvy4rrvuOlmtVsXGxmrp0qV59mtSUpLuuOMOhYaGKjAwUL169dL27dtdyjz11FOqXbu2AgICNGbMmCKdwVu+fLmuu+46l2WhoaEKDw93PD777DM1aNBA3bt3lyQlJyfrjTfe0Ny5c9WrVy+1a9dOixYt0vfff68ff/wx33b27NmjH3/8UQsWLFCHDh3UpEkTLViwQGlpaXr33Xddym7btk3PPfec3nzzzXy3dfXVV+vUqVPc0BkAAFQ5JEn5mTtXqlPnwo+//S1PVc/Bg6Xo6AvXvYjDmHbu3Knvv/9ePj4+jmWzZ8/W22+/rYULF+q3337T5MmTddtttzkOeBMSEtStWzf5+vrqm2++0datWzV69GhlZ2dLkl544QU999xzmjNnjnbs2KF+/frpb3/7W55hW9OnT9cjjzyiX375RV5eXrr11ls1depUvfDCC9q4caP27t2rxx57zKXON998o8OHD+vbb7/V3LlzNX36dF177bWqWbOmfvrpJ40fP1533nmnDh06JEnKyspSv379FBAQoI0bN2rTpk3y9/dX//79lZmZ6djuunXrFBcXp3Xr1umtt97S4sWLtXjxYsf6UaNG6eDBg1q3bp3ef/99vfLKKzp+/LhLbDfeeKOOHz+uL7/8Ulu3blXbtm3Vu3dvnTp1SpL073//WzNmzNCTTz6pLVu2KCIiQq+88kqh78+pU6f0+++/q3379gWWyczM1NKlSx1nBCVp69atysrKUp8+fRzlmjZtqrp16+qHH37IdzsZGRmSJD8/P8cyDw8P+fr66rvvvnMsO3v2rG699VbNnz9f4eHh+W7Lx8dHrVu31saNGwt9fQAAAJcaxtDkx2aTEhIuXC46Ou+yEydkKUpdm634cTn57LPP5O/vr+zsbGVkZMjDw0Mvv/yypHMHyk8++aTWrFmjTp06SZLq16+v7777Tq+++qq6d++u+fPnKygoSMuXL5e3t7ckqXHjxo7tz5kzR//4xz80bNgwSdLTTz+tdevWad68eZo/f76j3P33369+/fpJku655x7dcsstWrt2rbp06SJJGjNmjEuiIkm1atXSiy++KA8PDzVp0kTPPPOMzp49q4ceekiSNG3aND311FP67rvvNGzYML333nuy2+3617/+5UggFi1apBo1amj9+vXq27evJKlmzZp6+eWX5enpqaZNm2rgwIFau3atxo4dq927d+vLL7/Uzz//rA4dOkiS3njjDTVr1swR13fffaeff/5Zx48fl6+vr2M/rFy5Uu+//77GjRunefPmacyYMRozZowk6YknntCaNWsKPZt04MABGWMUGRlZYJmVK1cqKSlJf//73x3Ljh49Kh8fH9WoUcOlbO3atXX06NF8t5ObRE2bNk2vvvqqqlevrueff16HDh3SkSNHHOUmT56szp076/rrry8wJkmKjIxUfHx8oWUAAAAuNSRJ+QkMlKKiLlwuNDTvspAQmago5X+Vx3ltlELPnj21YMECnTlzRs8//7y8vLw0dOhQSdLevXt19uxZXX311S51MjMz1aZNG0nnhlp17drVkSA5s9lsOnz4sCPRydWlS5c8Q89atmzp+H/t2rUlSZdffrnLsvPP1rRo0UIeHh4uZS677DLHc09PTwUHBzvqbd++XXv37lVAQIDLdtLT0x3DA3O36+np6XgeERGhX3/9VZL0xx9/yMvLS+3atXOsb9q0qUsCsn37dqWmpio4ONilnbS0NEc7f/zxh8aPH++yvlOnTlq3bp0KkpaWJsn17M753njjDQ0YMKDQRKoovL299eGHH2rMmDGqVauWPD091adPHw0YMEDGGEnSJ598om+++Ub/+c9/Lrg9q9Wqs2fPliomAEAl8FEdKS1BskZJgw+5OxqgYHXqnDuZERUlHSq7vkqSlJ8pU849SiDno4/OXeRewMXwF0v16tXVsGFDSdKbb76pVq1a6Y033tCYMWOUmpoqSfr8888VdV6yl3uGxGq1XpQ4nJOs3LM85y+z2+0F1sktk9+y3Hqpqalq165dvtcQhTolqoVtoyhSU1MVERGh9evX51l3/tmc4ggJCZEknT592iXeXPHx8VqzZo0++OADl+Xh4eHKzMxUUlKSS/vHjh0rcIicJLVr107btm1TcnKyMjMzFRoaqo4dOzqG+33zzTeKi4vL85qGDh2qrl27urz+U6dOqUGDBsV8xQAAAJUb1yRdAjw8PPTQQw/pkUceUVpampo3by5fX18dOHBADRs2dHlE/3eIYMuWLbVx40aXiQ1yBQYGKjIyMs8Maps2bVLz5s3L5TU5a9u2rfbs2aOwsLA8rycoKKhI22jatKmys7O1detWx7Jdu3YpKSnJpZ2jR4/Ky8srTzu5iU6zZs30008/uWy7oEkUcjVo0ECBgYH6/fff812/aNEihYWFaeDAgS7L27VrJ29vb61du9Yl5gMHDjiGURYmKChIoaGh2rNnj7Zs2eIYWvfggw9qx44d2rZtm+MhSc8//3yeCTN27tzpOPsIAABQVZAkXSJuvPFGeXp6av78+QoICND999+vyZMn66233lJcXJx++eUXvfTSS3rrrbckSZMmTZLNZtOwYcO0ZcsW7dmzR0uWLNGuXbskSQ888ICefvppvffee9q1a5cefPBBbdu2Tffcc0+5v7bhw4crJCRE119/vTZu3Kh9+/Zp/fr1uvvuux2TO1xIkyZN1L9/f91555366aeftHXrVt1xxx0uZ9T69OmjTp06adCgQfr666+1f/9+ff/993r44Ye1ZcsWSeeuu3rzzTe1aNEi7d69W9OnT9dvv/1WaNseHh7q06ePy8QJuex2uxYtWqSRI0fmmWY7KChIY8aM0ZQpU7Ru3Tpt3bpVt99+uzp16qQrr7zSUa5p06b66KOPHM9XrFih9evX66+//tLHH3+sq6++WoMGDXJcuxUeHq7LLrvM5SFJdevWVWxsrGM7+/fvV0JCgsvEEQAAAFUBSdIlwsvLS5MmTdIzzzyjM2fOaNasWXr00Uc1e/ZsNWvWTP3799fnn3/uOAgODg7WN998o9TUVHXv3l3t2rXT66+/7hiydvfdd2vKlCm67777dPnll2vVqlX65JNP1KhRo3J/bdWqVdO3336runXrasiQIWrWrJlj6u3AYlzbtWjRIkVGRqp79+4aMmSIxo0bp7CwMMd6i8WiL774Qt26ddPtt9+uxo0ba9iwYYqPj3dcb3XzzTfr0Ucf1dSpU9WuXTvFx8drwoQJF2z7jjvu0PLly/MM/1uzZo0OHDig0aNH51vv+eef17XXXquhQ4eqW7duCg8P14cffuhSZteuXUpOTnY8P3LkiEaMGKGmTZvq7rvv1ogRI/JM/10U7777rvr27etybycAAICqwGJyr+a+RNlsNgUFBSk5OTnPAXV6err27dun2NjYQi+qLypjjLKzs+Xl5VXgDTpRNRlj1LFjR02ePFm33HJLgWUqSv/JzMxUo0aNtGzZsjwTeDi72J8hlIzdbtfx48cVFhbmMikKSi8uLk5jJt6toVP/qfCY2AtXcHI0fp8+eOZhvTH/xQp9bR/9B5JKPHED/QelUaL+U8qJGwrLDZzRm4FyYLFY9NprrznuQ1XRHThwQA899FChCRIAAMClitntgHLSunVrtW7d2t1hFEnuhBUAAABVEWeSAAAAAMAJSRIAAAAAOGG4nc5dMA+g+PjsAMAlovM7Uk6G5Onr7kiAwr3zjpSRIfmWbV+t0klS7nTXZ8+edblfDoCiyczMlCR5enq6ORIAQKnU7uHuCICi6dGjXJqp0kmSp6enatSooePHj0s6dz+e0ky9XJGmcEblU9n6j91uV2JioqpVq5bnRrgAAACVWZU/sgkPD5ckR6JUGsYY2e12eXh4VIqDXFQslbH/eHh4qG7dupUmXgAAgKKo8kmSxWJRRESEwsLClJWVVapt2e12nTx5UsHBwdxQDcVWGfuPj49PpYkVAFCIY+v/d00SQ+9Qka1f/79rkspw6F2VT5JyeXp6lvq6CrvdLm9vb/n5+XHgiGKj/wAA3Ob726S0BMkaJQ0+5O5ogILddpuUkCBFRUmHyq6vciQGAAAAAE5IkgAAAADACUkSAAAAADghSQIAAAAAJyRJAAAAAOCEJAkAAAAAnJAkAQAAAIATkiQAAAAAcEKSBAAAAABOvNwdAAAAANxs8CF3RwAUzaHy6aucSQIAAAAAJyRJAAAAAOCEJAkAAAAAnHBNEgAAQFX360wpM1nyCZIun+7uaICCzZwpJSdLQUHS9LLrqyRJAAAAVd3e16W0BMkaRZKEiu3116WEBCkqqkyTJIbbAQAAAIATkiQAAAAAcEKSBAAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJxwM1kAAICqLqy7lHFC8g1xdyRA4bp3l06ckELKtq+SJAEAAFR1XZa6OwKgaJaWT19luB0AAAAAOCFJAgAAAAAnJEkAAAAA4IRrkgAAAKq6tb2k9GOSX22p9zfujgYoWK9e0rFjUu3a0jdl11dJkgAAAKo6224pLUHKTHZ3JEDhdu+WEhKk5LLtqwy3AwAAAAAnJEkAAAAA4IQkCQAAAACckCQBAAAAgBOSJAAAAABwwux2AIBKJTExUTabrdj1AgMDFRoaWgYRAQAuNSRJAIBKIzExUaPHjVdKWnqx6wZY/fTmawtJlAAAF0SSBACoNGw2m1LS0tVjxAQFR9Qpcr2TRw5p/ZIFstlsJEkAgAsiSQIAVDrBEXUUHhPr7jCAS8flj0lZqZK3v7sjAQr32GNSaqrkX7Z9lSQJAACgqms4zt0RAEUzrnz6KrPbAQAAAIATkiQAAAAAcMJwOwAAgKou7YhkciSLp2SNcHc0QMGOHJFyciRPTymi7PoqSRIAAEBVt6qDlJYgWaOkwYfcHQ1QsA4dpIQEKSpKOlR2fZXhdgAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHBCkgQAAAAATkiSAAAAAMCJW5Ok2bNnq0OHDgoICFBYWJgGDRqkXbt2uZRJT0/XxIkTFRwcLH9/fw0dOlTHjh1zU8QAAAAALnVuTZI2bNigiRMn6scff9Tq1auVlZWlvn376syZM44ykydP1qeffqoVK1Zow4YNOnz4sIYMGeLGqAEAAABcytx6M9lVq1a5PF+8eLHCwsK0detWdevWTcnJyXrjjTe0bNky9erVS5K0aNEiNWvWTD/++KOuvPJKd4QNAAAA4BLm1iTpfMnJyZKkWrVqSZK2bt2qrKws9enTx1GmadOmqlu3rn744Yd8k6SMjAxlZGQ4nttsNkmS3W6X3W4vy/Blt9tljCnzdnBpov+gpKpS3zHGyGKxSMZIphiv1xhlZWVq//79MsYUuVp8fLxycnKK395/27RYLBX+valK/QeF6LlaMtmSxUsqRl+g/6A0StR/Vq+WsrMlr+L1Vec2i6LCJEl2u1333nuvunTpossuu0ySdPToUfn4+KhGjRouZWvXrq2jR4/mu53Zs2dr5syZeZYnJiYqPT39osftzG63Kzk5WcYYeXgwJwaKh/6DkqpKfSclJUWxdaNlzUmTJeVUkeuZ5ERV9/PT4qXvytvbu8j1MjIy5F+tmrwzUorVniRZc9IUWzdaKSkpOn78eLHqlqeq1H9QmJr/+2960fsr/QelUaL+U9Opr5bguzUlJaVI5SpMkjRx4kTt3LlT3333Xam2M23aNE2ZMsXx3GazKTo6WqGhoQoMDCxtmIWy2+2yWCwKDQ3liwLFRv9BSVWlvpOamqp9Bw6qjadVQQG1ilzvxNnf9MeevWo7dJSiYuoXud7e7Vv07cK56mW8FFyM9iQp7ZRN+w4cdExOVFFVpf6Di4/+g9JwR//x8/MrUrkKkSRNmjRJn332mb799lvVqVPHsTw8PFyZmZlKSkpyOZt07NgxhYeH57stX19f+fr65lnu4eFRLjvfYrGUW1u49NB/UFJVpe/kDl+TxSJZivNaLbLb7QoOj1R4vaInSYlHDp0bmlHs9iT9N9bc96Yiqyr9B2WD/oPSKO/+U9R23JokGWN011136aOPPtL69esVGxvrsr5du3by9vbW2rVrNXToUEnSrl27dODAAXXq1MkdIQMAAFx69i+Tss9KXtWkere6OxqgYMuWSWfPStWqSbeWXV91a5I0ceJELVu2TB9//LECAgIc1xkFBQXJarUqKChIY8aM0ZQpU1SrVi0FBgbqrrvuUqdOnZjZDgAA4GL5z1QpLUGyRpEkoWKbOlVKSJCioi7dJGnBggWSpB49ergsX7RokUaNGiVJev755+Xh4aGhQ4cqIyND/fr10yuvvFLOkQIAAACoKtw+3O5C/Pz8NH/+fM2fP78cIgIAAABQ1XGFHQAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHDi1inAAQAAUAFYw11/AhVVeLjrzzJCkgQAAFDV9d/i7giAotlSPn2V4XYAAAAA4IQzSQBQQSUmJspms12wnDFGKSkpSk1NlcViUWBgoEJDQ8shwnOKGuf5yjtOAACKiiQJACqgxMREjR43Xilp6Rcsa7FYFFs3WvsOHJQxRgFWP7352sJySUCKE+f5yjNOAACKgyQJACogm82mlLR09RgxQcERdQovbIysOWlq42nVyaMJWr9kgWw2W7kkH8WK08nJI4fKNU4AF/DznVLGKcm3lnTFq+6OBijYnXdKp05JtWpJr5ZdXyVJAoAKLDiijsJjYgsvZOyypJxSUEAtyWIpn8DOU6Q4AVRcCZ9LaQmSNcrdkQCF+/xzKSFBiirbvsrEDQAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHBCkgQAAAAATkiSAAAAAMAJSRIAAAAAOCFJAgAAAAAn3EwWAACgqqt3i5R5WvKp6e5IgMLdcot0+rRUs2z7KkkSAABAVdfmWXdHABTNs+XTVxluBwAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAACo6j5rKv078NxPoCJr2lQKDDz3swwxcQMAQJKUmJgom81WrDrx8fHKzs4uUXuZmRmKj48vt/YAFCIrVcpOOfcTqMhSU6WUlHM/yxBJEgBAiYmJGj1uvFLS0otVL+3sGR0+ekxZWZnFqpeSdEr74v7Sw7OelK+vb5m3BwBAcZAkAQBks9mUkpauHiMmKDiiTpHr7dm2WR+8Mkc5OTnFai/97Bl5eHur+4gJiqrXoMzbAwCgOEiSAAAOwRF1FB4TW+TyiYcPlq698MhybQ8AgKJg4gYAAAAAcEKSBAAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACfMbgcAAFDVXbFQykmTPK3ujgQo3MKFUlqaZC3bvkqSBAAAUNVFXevuCICiubZ8+irD7QAAAADACUkSAAAAADhhuB0AAEBVd2qrlJMpefpItdq5OxqgYFu3SpmZko+P1K7s+ipJEgAAQFW34XopLUGyRkmDD7k7GqBg118vJSRIUVHSobLrqwy3AwAAAAAnJEkAAAAA4IQkCQAAAACckCQBAAAAgBOSJAAAAABwQpIEAAAAAE5IkgAAAADACUkSAAAAADghSQIAAAAAJ17uDgAAAABudu0fkowki7sjAQr3xx+SMZKlbPsqSRIAAEBV5x3g7giAogkon77KcDsAAAAAcEKSBAAAAABOGG4HAABQ1f0xV8qySd6BUrMp7o4GKNjcuZLNJgUGSlPKrq+SJAEAAFR1f86V0hIkaxRJEiq2uXOlhAQpKqpMkySG2wEAAACAE5IkAAAAAHBCkgQAAAAATkiSAAAAAMAJSRIAAAAAOGF2OwAAKpjExETZbLZi1wsMDFRoaGgZRAQAVQtJEgAAFUhiYqJGjxuvlLT0YtcNsPrpzdcWkigBQCmRJAEAUIHYbDalpKWrx4gJCo6oU+R6J48c0volC2Sz2UiSAKCUSJIAAKiAgiPqKDwm1t1hoKqo1VZKj5b8SLBRwbVtK0VHS2X8xyCSJAAAgKqu+yfujgAomk/Kp68yux0AAAAAOCFJAgAAAAAnJEkAAAAA4IRrkgAAAKq6DX+T0hPPTdzA9UmoyP72Nykx8dzEDWV4fRJJEgAAQFV36hcpLUGyRrk7EqBwv/wiJSRIUWXbVxluBwAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACckSQAAAADgxK1J0rfffqvrrrtOkZGRslgsWrlypcv6UaNGyWKxuDz69+/vnmABAAAAVAluTZLOnDmjVq1aaf78+QWW6d+/v44cOeJ4vPvuu+UYIQAAAICqxq33SRowYIAGDBhQaBlfX1+Fh4eXU0QAAAAAqroKfzPZ9evXKywsTDVr1lSvXr30xBNPKDg4uMDyGRkZysjIcDy32WySJLvdLrvdXqax2u12GWPKvB1cmug/cGaMkcVikYyRzAX6hDEuD4vFUuy+VKz2XGvKw8ODevlWLef3ooTtueO758SJE47fz8URGBiokJCQMogIajJZliybjHegVMH7Dy4dJeo/kyfLYrPJBBavrzq3WRQVOknq37+/hgwZotjYWMXFxemhhx7SgAED9MMPP8jT0zPfOrNnz9bMmTPzLE9MTFR6enqZxmu325WcnCxj/vtLFSgG+g+cpaSkKLZutKw5abKknLpAaSNLWopkkaw5aYqtG62UlBQdP368jNr7nyBPqUXTJqpuMql3nvJ+L0raXnl/9yQnJ+vFl19RWmZmsetafXx096T/U1BQUBlEVsXVGv6//1fg/oNLS4n6z/CS9dVcKSkpRSpXoZOkYcOGOf5/+eWXq2XLlmrQoIHWr1+v3r1751tn2rRpmjJliuO5zWZTdHS0QkNDFRgYWKbx2u12WSwWhYaG8kWBYqP/wFlqaqr2HTioNp5WBQXUKrywMZKRjH8tpZ1K0b4DBxUQEKCwsLCyac9Jco7025+71NviI0M9F2mnbOX6XpS0vfL+7klNTdXve+PU/bY7FRxep8j1Th49pA3vvCpPT89ivT6ULX53oTTc0X/8/PyKVK5CJ0nnq1+/vkJCQrR3794CkyRfX1/5+vrmWe7h4VEuO99isZRbW7j00H+QK3fYlCwWyXKh/mD/b7lzj9zhWsXpR8Vrz6XmuaEL1Munajm/FyVsL7fN8vw9aYxRcES0wmNii1OxxK8PZYvfXSiN8u4/RW2nUiVJhw4d0smTJxUREeHuUAAAAC4dWSmSjCSL5B3g7miAgqWknBtBYbFIAWXXV92aJKWmpmrv3r2O5/v27dO2bdtUq1Yt1apVSzNnztTQoUMVHh6uuLg4TZ06VQ0bNlS/fv3cGDUAAMAl5rNmUlqCZI2SBh9ydzRAwZo1kxISpKgo6VDZ9VW3JklbtmxRz549Hc9zryUaOXKkFixYoB07duitt95SUlKSIiMj1bdvX82aNSvf4XQAAAAAcDG4NUnq0aPHuXHXBfjqq6/KMRoAAAAAkLjCDgAAAACckCQBAAAAgBOSJAAAAABwQpIEAAAAAE5IkgAAAADACUkSAAAAADghSQIAAAAAJ269TxIAAAAqgO4fSzmZkqePuyMBCvfxx1JmpuRTtn21RElS/fr1tXnzZgUHB7ssT0pKUtu2bfXXX39dlOAAAABQDmq1c3cEQNG0K5++WqIkaf/+/crJycmzPCMjQwkJCaUOCgAAlJ/k5GSlpqbKYrEUq15gYKBCQ0PLKCoAcJ9iJUmffPKJ4/9fffWVgoKCHM9zcnK0du1a1atX76IFBwAAytaJEyf04suv6Pe9cTLGFKtugNVPb762kEQJwCWnWEnSoEGDJEkWi0UjR450Weft7a169erpueeeu2jBAQCAsmWz2ZSWmanut92p4IjoItc7eeSQ1i9ZIJvNRpJ0KUj4TMpJkzytUtS17o4GKNhnn0lpaZLVKl1bdn21WEmS3W6XJMXGxmrz5s0KCQkpk6AAAED5Cg6vo/CYWHeHAXf5ebyUliBZo6TBh9wdDVCw8eOlhAQpKko6VHZ9tUTXJO3bt+9ixwEAAAAAFUKJpwBfu3at1q5dq+PHjzvOMOV68803Sx0YAAAAALhDiZKkmTNn6vHHH1f79u0VERFR7NlwAAAAAKCiKlGStHDhQi1evFgjRoy42PEAAAAAgFt5lKRSZmamOnfufLFjAQAAAAC3K1GSdMcdd2jZsmUXOxYAAAAAcLsSDbdLT0/Xa6+9pjVr1qhly5by9vZ2WT937tyLEhwAAAAAlLcSJUk7duxQ69atJUk7d+50WcckDgAAAAAqsxIlSevWrbvYcQAAAMBdvP2lrIBzP4GKzN9fCgg497MMlfg+SQAAALhEXPunuyMAiubP8umrJUqSevbsWeiwum+++abEAQEAAACAO5UoScq9HilXVlaWtm3bpp07d2rkyJEXIy4AAAAAcIsSJUnPP/98vstnzJih1NTUUgUEAAAAAO50Ua9Juu2223TFFVdozpw5F3OzAAAAKEv/eUDKPC351JTaPOvuaICCPfCAdPq0VLOm9GzZ9dWLmiT98MMP8vPzu5ibBICLLjExUTabrdj1AgMDFRoaWgYRXVyZmRmKj48vVp34+HhlZ2eXUURVF+8FKo3970ppCZI1iiQJFdu770oJCVJUVMVLkoYMGeLy3BijI0eOaMuWLXr00UcvSmAAUBYSExM1etx4paSlF7tugNVPb762sEInSilJp7Qv7i89POtJ+fr6Frle2tkzOnz0mLKyMsswuqqF9wIAKq8SJUlBQUEuzz08PNSkSRM9/vjj6tu370UJDADKgs1mU0paunqMmKDgiDpFrnfyyCGtX7JANputQidJ6WfPyMPbW91HTFBUvQZFrrdn22Z98Moc5eTklGF0VQvvBQBUXiVKkhYtWnSx4wCAchUcUUfhMbHuDqPMBIdHFuv1JR4+WIbRVG28FwBQ+ZTqmqStW7fqjz/+kCS1aNFCbdq0uShBAQAAAIC7lChJOn78uIYNG6b169erRo0akqSkpCT17NlTy5cvr9BDUQAAAACgMB4lqXTXXXcpJSVFv/32m06dOqVTp05p586dstlsuvvuuy92jAAAAABQbkp0JmnVqlVas2aNmjVr5ljWvHlzzZ8/n4kbAAAAAFRqJTqTZLfb5e3tnWe5t7e37HZ7qYMCAAAAAHcp0ZmkXr166Z577tG7776ryMhISVJCQoImT56s3r17X9QAAQAAUMaiBkoZpyTfWu6OBCjcwIHSqVNSrbLtqyVKkl5++WX97W9/U7169RQdHS1JOnjwoC677DK98847FzVAAAAAlLErXnV3BEDRvFo+fbVESVJ0dLR++eUXrVmzRn/++ackqVmzZurTp89FDQ4AAAAAyluxrkn65ptv1Lx5c9lsNlksFl199dW66667dNddd6lDhw5q0aKFNm7cWFaxAgAAAECZK1aSNG/ePI0dO1aBgYF51gUFBenOO+/U3LlzL1pwAAAAAFDeipUkbd++Xf379y9wfd++fbV169ZSBwUAAIBytKq99FGdcz+Biqx9e6lOnXM/y1Cxrkk6duxYvlN/Ozbm5aXExMRSBwUAAIBylHZUSktwdxTAhR09KiWUfV8t1pmkqKgo7dy5s8D1O3bsUERERKmDAgAAAAB3KVaSdM011+jRRx9Venp6nnVpaWmaPn26rr322osWHAAAAACUt2INt3vkkUf04YcfqnHjxpo0aZKaNGkiSfrzzz81f/585eTk6OGHHy6TQAGgskpMTJTNZitWnfj4eGVnZ5dRRLhUZWZmKD4+vlh14uPjlZOTU0YRAUDlVKwkqXbt2vr+++81YcIETZs2TcYYSZLFYlG/fv00f/581a5du0wCBYDKKDExUaPHjVdKWt4z8IVJO3tGh48eU1ZWZhlFhktNStIp7Yv7Sw/PelK+vr5FrpeedlY1a9SkrwGAk2LfTDYmJkZffPGFTp8+rb1798oYo0aNGqlmzZplER8AVGo2m00paenqMWKCgiPqFLnenm2b9cErc/gLP4os/ewZeXh7q/uICYqq16DI9fZu26xd6z+nrwGAk2InSblq1qypDh06XMxYAOCSFRxRR+ExsUUun3j4YBlGg0tZcHhkMfvagTKMBgAqp2JN3AAAAAAAlzqSJAAAAABwUuLhdgAAALhEtHlGyj4reVVzdyRA4Z55Rjp7VqpWtn2VJAkAAKCqq3eruyMAiubW8umrDLcDAAAAACckSQAAAADghOF2AAAAVZ1tl2TPljy8pMAm7o4GKNiuXVJ2tuTlJTUpu75KkgQAAFDVre0tpSVI1ihp8CF3RwMUrHdvKSFBioqSDpVdX2W4HQAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAE2a3A4AiyszMUHx8fLHqxMfHKzs7u4wiAqqWknwGJSkwMFChoaHFrpeYmCibzVZu7QGoOEiSAKAIUpJOaV/cX3p41pPy9fUtcr20s2d0+OgxZWVllmF0wKWvpJ9BSQqw+unN1xYWK3FJTEzU6HHjlZKWXtxQS9QegIqFJAkAiiD97Bl5eHur+4gJiqrXoMj19mzbrA9emaOcnJwyjA649JX0M3jyyCGtX7JANputWEmLzWZTSlq6eoyYoOCIOmXeHoCKhSQJAIohODxS4TGxRS6fePhgGUYDVD3F/QyWur2IOuXaHoCKgSQJAACgquu/WTI5ksXT3ZEAhdu8WcrJkTzLtq+SJAEAAFR11gh3RwAUTUT59FWmAAcAAAAAJyRJAAAAAOCE4XYAAABV3d7XpKxUydtfajjO3dEABXvtNSk1VfL3l8aVXV8lSQIAAKjqfn1cSkuQrFEkSajYHn9cSkiQoqLKNEliuB0AAAAAOCFJAgAAAAAnJEkAAAAA4IQkCQAAAACckCQBAAAAgBO3JknffvutrrvuOkVGRspisWjlypUu640xeuyxxxQRESGr1ao+ffpoz5497gkWAAAAQJXg1iTpzJkzatWqlebPn5/v+meeeUYvvviiFi5cqJ9++knVq1dXv379lJ6eXs6RAgAAAKgq3HqfpAEDBmjAgAH5rjPGaN68eXrkkUd0/fXXS5Lefvtt1a5dWytXrtSwYcPKM1QAAAAAVUSFvZnsvn37dPToUfXp08exLCgoSB07dtQPP/xQYJKUkZGhjIwMx3ObzSZJstvtstvtZRqz3W6XMabM28Glif5TPowxslgskjGSKc6+NvLw8KiY9Yz536Mix1ml6lWmWEv4mfjvZ6m431vl/hks7zhL2J67WQIaS95Bkl9tmWLEze8ulEZJ+o+lcWMpKEiqXby+6txmUVTYJOno0aOSpNq1a7ssr127tmNdfmbPnq2ZM2fmWZ6YmFjmw/TsdruSk5NlzH+/yIFioP+Uj5SUFMXWjZY1J02WlFNFrhfkKbVo2kTVTWYFrGdkSUuRLBU9zqpTrzLFGuQp1a1Tp9j1rDlpiq0brZSUFB0/frzI9cr7M1jecZa0Pbe7bNn//l+MuPndhdIoUf9ZVrK+mislJaVI5SpsklRS06ZN05QpUxzPbTaboqOjFRoaqsDAwDJt2263y2KxKDQ0lC8KFBv9p3ykpqZq34GDauNpVVBArSLXS86Rfvtzl3pbfGQqWj1jJCMZ/1oVO84qVK8yxZqcIx04dEgNi1kv7ZRN+w4cVEBAgMLCwopcr7w/g+UdZ0nbq6z43YXScEf/8fPzK1K5CpskhYeHS5KOHTumiIgIx/Jjx46pdevWBdbz9fWVr69vnuUeHh7lsvMtFku5tYVLD/2n7OUOg5HFIlmKs58t507RV8h6ueUsFTzOqlSvMsVaws/Efz9Lud9bRa9Wzp/B8o6zhO1VZvzuQmmUd/8pajsVtjfHxsYqPDxca9eudSyz2Wz66aef1KlTJzdGBgAAAOBS5tYzSampqdq7d6/j+b59+7Rt2zbVqlVLdevW1b333qsnnnhCjRo1UmxsrB599FFFRkZq0KBB7gsaAADgUrNpuJRxQvINkbosdXc0QMGGD5dOnJBCQqSlZddX3ZokbdmyRT179nQ8z72WaOTIkVq8eLGmTp2qM2fOaNy4cUpKStJVV12lVatWFXksIQAAAIrg+AYpLUGyRrk7EqBwGzZICQlSVNn2VbcmST169Dg33rcAFotFjz/+uB5//PFyjAoAAABAVVZhr0kCAAAAAHcgSQIAAAAAJyRJAAAAAOCEJAkAAAAAnJAkAQAAAIATkiQAAAAAcEKSBAAAAABO3HqfJAAAAFQADcdKmcmST5C7IwEKN3aslJwsBZVtXyVJAgAAqOoun+7uCICimV4+fZXhdgAAAADghCQJAAAAAJyQJAEAAACAE5IkAACAqu6jOtIyy7mfQEVWp45ksZz7WYZIkgAAAADACUkSAAAAADghSQIAAAAAJyRJAAAAAOCEJAkAAAAAnJAkAQAAAIATkiQAAAAAcEKSBAAAAABOSJIAAAAAwImXuwMAAACAm3V+R8rJkDx93R0JULh33pEyMiTfsu2rJEkAAABVXe0e7o4AKJoePcqlGYbbAQAAAIATkiQAAAAAcMJwOwAAgKru2Pr/XZPE0DtUZOvX/++apDIcekeSBAAALmmZmRmKj48vVp34+HhlZ2eXUUT5S0xMlM1mK1HdwMBAhYaGlrzx72+T0hIka5Q0+FDJtwOUtdtukxISpKgo6VDZ9VWSJAAAcMlKSTqlfXF/6eFZT8q3GLNhpZ09o8NHjykrK7MMo/ufxMREjR43Xilp6SWqH2D105uvLSxdogTAgSQJAABcstLPnpGHt7e6j5igqHoNilxvz7bN+uCVOcrJySnD6P7HZrMpJS1dPUZMUHBEnWLVPXnkkNYvWSCbzUaSBFwkJEkAAOCSFxweqfCY2CKXTzx8sAyjKVhwRJ1ixQmgbDC7HQAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHDCFOAAAABV3eBD7o4AKJpD5dNXOZMEAAAAAE5IkgAAAADACUkSAAAAADjhmiQAAICq7teZUmay5BMkXT7d3dEABZs5U0pOloKCpOll11dJkgAAAKq6va9LaQmSNYokCRXb669LCQlSVFSZJkkMtwMAAAAAJyRJAAAAAOCE4XZAOUlMTJTNZitwvTFGKSkpSk1NlcVicSwPDAxUaGhoeYQIAMWSmZmh+Pj4YtWJj49XdnZ2GUUEABcHSRJQDhITEzV63HilpKUXWMZisSi2brT2HTgoY4xjeYDVT2++tpBECUCFkpJ0Svvi/tLDs56Ur69vkeulnT2jw0ePKSsrswyjA4DSIUkCyoHNZlNKWrp6jJig4Ig6+RcyRtacNLXxtEr/PZN08sghrV+yQDabjSQJQIWSfvaMPLy91X3EBEXVa1Dkenu2bdYHr8xRTk5OGUYHAKVDkgSUo+CIOgqPic1/pbHLknJKQQG1JAuXCwKoHILDIwv+XstH4uGDZRgNAFwcHIkBAAAAgBOSJAAAAABwwnA7AACAqi6su5RxQvINcXckQOG6d5dOnJBCyravkiQBAABUdV2WujsCoGiWlk9fZbgdAAAAADghSQIAAAAAJyRJAAAAAOCEa5IAAACqurW9pPRjkl9tqfc37o4GKFivXtKxY1Lt2tI3ZddXSZIAAACqOttuKS1Bykx2dyRA4XbvlhISpOSy7asMtwMAAAAAJyRJAAAAAOCEJAkAAAAAnJAkAQAAAIATJm4A4CIxMVE2m63Y9QIDAxUaGlrh2wOAS1FmZobi4+NLUC9TPj4+isnOlpek7OxsxcfFXbAe38G41JEkAXBITEzU6HHjlZKWXuy6AVY/vfnawmL90izv9gDgUpSSdEr74v7Sw7OelK+vb5HrZWZm6OD+/Yqp30Af3Zys0OrS6aRkjZl49wXr5n4HBwcHlyZ0oMIiSQLgYLPZlJKWrh4jJig4ok6R6508ckjrlyyQzWYrVtJS3u0BwKUo/ewZeXh7q/uICYqq16DI9fZs26z4V+boqlvHyVptm2RPlzUgUEOn/rPQes7fwSRJuFSRJAHIIziijsJjYi/Z9gDgUhQcHlms79LEwwcd9TzOeEl2ycPTi+9jQCRJAAAAVd7vdabKKydV2Z7+7g4FKNxjj0mpqZJ/2fZVkiQAAIAqbl/YKHeHABTNuHHl0gxTgAMAAACAE5IkAAAAAHDCcDsAAIAqzi/zqCwmR8biqXSfcHeHAxTsyBEpJ0fy9JQiIsqsGZIkAACAKq73zl6qlnVYZ70j9Xnb390dDlCwDh2khAQpKko6dKjMmmG4HQAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAkwqdJM2YMUMWi8Xl0bRpU3eHBQAAAOASVuGnAG/RooXWrFnjeO7lVeFDBgAAAFCJVfiMw8vLS+Hh3NQMAAAAQPmo8EnSnj17FBkZKT8/P3Xq1EmzZ89W3bp1CyyfkZGhjIwMx3ObzSZJstvtstvtZRqr3W6XMabM20HlY4yRxWKRjJFMAf3DmP89ZHcss1gs5davihRn/hVLFGdp2svKytT+/ftljCl6PUnx8fHKyckpfpsy8vDwqJj1XPpOBY6zStWrTLGW8HNYiV5fudYr7+/D0sTqXE/Gsawo3zm538F2u10pKSlKSUk5F/8FBAYGKiQkpBgx4lJWkmNny38fRpIpwbFRUduq0ElSx44dtXjxYjVp0kRHjhzRzJkz1bVrV+3cuVMBAQH51pk9e7ZmzpyZZ3liYqLS09PLNF673a7k5GQZ898vHeC/UlJSFFs3WtacNFlSThVQysiSlnLuk3/uH1lz0hRbN1opKSk6fvx4BYkzr5LGWdL2THKiqvv5afHSd+Xt7V3ketK5P6T4V6sm74yUYrUZ5Cm1aNpE1U1mBaz3v75TseOsOvUqU6xBnlLdOnUqRZyVoV55fx+WJlbnet9GvyUPZcsurwtuw+U72MdHtUNDdCzxxH+TrcJZfXx096T/U1BQUJHjxKWrJMfOnsuXy5KTI+PpqZwSHBulpKQUqVyFTpIGDBjg+H/Lli3VsWNHxcTE6N///rfGjBmTb51p06ZpypQpjuc2m03R0dEKDQ1VYGBgmcZrt9tlsVgUGhpKkgQXqamp2nfgoNp4WhUUUCv/Qsac+wOefy3pv3+NSztl074DBxUQEKCwsLCKEWc+ShpnSds7cfY3/bFnr9oOHaWomPpFridJe7dv0bcL56qX8VJwMdpMzpF++3OXelt8ZCpaPae+U6HjrEL1KlOsyTnSgUOH1LASxFkZ6pX392FpYnWulxLSqsj1XL6D68bKmpOmSE+r43dXQU4ePaQN77wqT0/PcvmdhoqvRMfOpew7fn5+RSpXoZOk89WoUUONGzfW3r17Cyzj6+srX1/fPMs9PDzKJXGxWCzl1hYqj9yhF7JYJEtBfcP+3/VOZf5bL7dfVYw4861YojhL3J4sstvtCg6PVHi94iVJiUcOnTvVXsI2K2Y9p75ToeOsSvUqU6yl+xxS7/xq5f19WIpYS1nv3HdwrCwpp84ldhfaRjn/TkPlUN7HzkVtp1L10NTUVMXFxSkiIsLdoQAAAAC4RFXoM0n333+/rrvuOsXExOjw4cOaPn26PD09dcstt7g7NAAAgEtG9IkV8rKnKdvDqoMhN7o7HKBgy5ZJZ89K1apJt95aZs1U6CTp0KFDuuWWW3Ty5EmFhobqqquu0o8//qjQ0FB3hwYAAHDJaHlguqplHdZZ70iSJFRsU6dKCQlSVFTVTZKWL1/u7hAAAAAAVDGV6pokAAAAAChrJEkAAAAA4IQkCQAAAACckCQBAAAAgBOSJAAAAABwQpIEAAAAAE5IkgAAAADASYW+TxIAAADKXrpPmMtPoMIKD3f9WUZIkgAAAKq4tZetd3cIQNFs2VIuzZAkAbgoMjMzFB8fX6w68fHxys7OLqOIAMA9+D4EKj+SJACllpJ0Svvi/tLDs56Ur69vkeulnT2jw0ePKSsrswyjA4Dyw/chcGkgSQJQaulnz8jD21vdR0xQVL0GRa63Z9tmffDKHOXk5JRhdABQfvg+BC4NJEkALprg8EiFx8QWuXzi4YNlGA0AuE9l+z5su+9e+WSfVqZXTf0SO8+tsQCFuvNO6dQpqVYt6dVXy6wZkiQAAIAqLuL016qWdVhnvSOloud2QPn7/HMpIUGKiirTZrhPEgAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHBCkgQAAAAATkiSAAAAAMAJN5MFAACo4g6GDJV3dpKyvGq4OxSgcLfcIp0+LdWsWabNkCQBAABUcTvqznJ3CEDRPPtsuTTDcDsAAAAAcEKSBAAAAABOSJIAAAAAwAnXJAEAAFRx/bZ3kDXzqNJ8wvVVq83uDiePxMRE2Wy2YtcLDAxUaGhohW8PxdC0qXT4sBQZKf35Z5k1Q5IEAABQxXnlnJG3PUVZOQHuDiWPxMREjR43Xilp6cWuG2D105uvLSxW4lLe7aGYUlOllJRzP8sQSRIAAAAqLJvNppS0dPUYMUHBEXWKXO/kkUNav2SBbDZbsZKW8m4PFRNJEgAAACq84Ig6Co+JvWTbQ8XCxA0AAAAA4IQkCQAAAACckCQBAAAAgBOSJAAAAABwQpIEAAAAAE5IkgAAAADACVOAAwAAVHG/xM6Vpz1dOR5+7g4FKNzChVJammS1lmkzJEkAAABV3JGa/d0dAlA0115bLs0w3A4AAAAAnJAkAQAAAIAThtsBAABUcTXObJOHPVN2Dx8lVW/t7nCAgm3dKmVmSj4+Urt2ZdYMSRIAAEAV12XXraqWdVhnvSP1edvf3R0OULDrr5cSEqSoKOnQoTJrhuF2AAAAAOCEJAkAAAAAnDDcDhVGYmKibDZbsesFBgYqNDS03NrLzMyUj49PserEx8crOzu72G2day9D8fHxJahXvnECAIBLX3kfr7kLSRIqhMTERI0eN14paenFrhtg9dObry0s1gevpO1lZmbo4P79iqnfQF5eRf/4pJ09o8NHjykrK7NY7aUkndK+uL/08Kwn5evrW2HjBAAAl77yPl5zJ5IkVAg2m00paenqMWKCgiPqFLneySOHtH7JAtlstmJ96Era3p5tmxX/yhxddes4RdVrUKx6H7wyRzk5OUWuI0npZ8/Iw9tb3UdMKHZ75RknAAC49JX38Zo7kSShQgmOqKPwmNgK217i4YPn6oVHlqheSZW0vfKOEwAAXPrK+3jNHZi4AQAAAACckCQBAAAAgBOSJAAAAABwwjVJAAAAVdxXrX6SjJEsFneHAhTujz/Kpa+SJAEAAFRx2Z4B7g4BKJqA8umrDLcDAAAAACckSQAAAADghOF2AAAAVVyjIy/LOydFWZ4B2hMxyd3hAAWbO1ey2aTAQGnKlDJrhiQJAACgimt85BVVyzqss96RJEmo2ObOlRISpKioMk2SGG4HAAAAAE5IkgAAAADACUkSAAAAADghSQIAAAAAJ0zcUM4SExNls9mKXS8wMFChoaFlEFH+KkucAACg8sjMzFB8fHyx6sTHxys7O7tStFdSHHdVPCRJ5SgxMVGjx41XSlp6sesGWP305msLy+WDUFniBAAAlUdK0inti/tLD896Ur6+vkWul3b2jA4fPaasrMwK3V5JcdxVMZEklSObzaaUtHT1GDFBwRF1ilzv5JFDWr9kgWw2W7l8CCpLnAAAoPJIP3tGHt7e6j5igqLqNShyvT3bNuuDV+YoJyenQrdXUhx3VUwkSW4QHFFH4TGx7g7jgipLnAAAoPIIDo8s1vFF4uGDlaq9kuK4q2IhSQIAAKjikqq3VFp2lDK8gt0dClC4tm2l6GipjM+ekSQBAABUcZuaLHd3CEDRfPJJuTTDFOAAAAAA4IQkCQAAAACckCQBAAAAgBOuSQIAAKjiuuwaJt/sk8rwCub6JFRsf/ublJh4buKGMrw+iSQJAACgiqtxZoeqZR3WWe9Id4cCFO6XX6SEBCkqqkybYbgdAAAAADghSQIAAAAAJyRJAAAAAOCEJAkAAAAAnFSKJGn+/PmqV6+e/Pz81LFjR/3888/uDgkAAADAJarCJ0nvvfeepkyZounTp+uXX35Rq1at1K9fPx0/ftzdoQEAAAC4BFX4JGnu3LkaO3asbr/9djVv3lwLFy5UtWrV9Oabb7o7NAAAAACXoAp9n6TMzExt3bpV06ZNcyzz8PBQnz599MMPP+RbJyMjQxkZGY7nycnJkqSkpCTZ7fYyjddut8tms8nHx0ceHnnzT5vNppycbB2O26W01JQib/f0scNKT0vTb7/9JpvNdjFDztfBgweVmZFRrnGWd5slbe/4gX2SMTq8b69MdvbFrWeMrDnpSvNMkCyWsm+vCtarTLEWq55T36nQcVahepUp1sQD+5WTna3D++NkcnIqbJzUK9s2k6yZyrZLqR6Z2vfbjqK3l5WV53dXWcZZkeuV9zFJeR8fSqWLNScnWzabTUlJSY7lFzp2zo/FbpdFkrHbZZy2VVS5+8oYU3g75kIl3Ojw4cOKiorS999/r06dOjmWT506VRs2bNBPP/2Up86MGTM0c+bM8gwTAAAAQCVy8OBB1alTp8D1FfpMUklMmzZNU6ZMcTy32+06deqUgoODZbnAXzhKy2azKTo6WgcPHlRgYGCZtoVLD/0HJUXfQWnQf1Aa9B+Uhjv6jzFGKSkpioyMLLRchU6SQkJC5OnpqWPHjrksP3bsmMLDw/Ot4+vrK19fX5dlNWrUKKsQ8xUYGMgXBUqM/oOSou+gNOg/KA36D0qjvPtPUFDQBctU6IkbfHx81K5dO61du9axzG63a+3atS7D7wAAAADgYqnQZ5IkacqUKRo5cqTat2+vK664QvPmzdOZM2d0++23uzs0AAAAAJegCp8k3XzzzUpMTNRjjz2mo0ePqnXr1lq1apVq167t7tDy8PX11fTp0/MM9wOKgv6DkqLvoDToPygN+g9KoyL3nwo9ux0AAAAAlLcKfU0SAAAAAJQ3kiQAAAAAcEKSBAAAAABOSJIAAAAAwAlJUjHNnj1bHTp0UEBAgMLCwjRo0CDt2rXLpUx6eromTpyo4OBg+fv7a+jQoXluiIuqacGCBWrZsqXjpmmdOnXSl19+6VhP30FRPfXUU7JYLLr33nsdy+g/KMiMGTNksVhcHk2bNnWsp+/gQhISEnTbbbcpODhYVqtVl19+ubZs2eJYb4zRY489poiICFmtVvXp00d79uxxY8SoKOrVq5fn+8disWjixImSKu73D0lSMW3YsEETJ07Ujz/+qNWrVysrK0t9+/bVmTNnHGUmT56sTz/9VCtWrNCGDRt0+PBhDRkyxI1Ro6KoU6eOnnrqKW3dulVbtmxRr169dP311+u3336TRN9B0WzevFmvvvqqWrZs6bKc/oPCtGjRQkeOHHE8vvvuO8c6+g4Kc/r0aXXp0kXe3t768ssv9fvvv+u5555TzZo1HWWeeeYZvfjii1q4cKF++uknVa9eXf369VN6erobI0dFsHnzZpfvntWrV0uSbrzxRkkV+PvHoFSOHz9uJJkNGzYYY4xJSkoy3t7eZsWKFY4yf/zxh5FkfvjhB3eFiQqsZs2a5l//+hd9B0WSkpJiGjVqZFavXm26d+9u7rnnHmMM3z0o3PTp002rVq3yXUffwYX84x//MFdddVWB6+12uwkPDzfPPvusY1lSUpLx9fU17777bnmEiErknnvuMQ0aNDB2u71Cf/9wJqmUkpOTJUm1atWSJG3dulVZWVnq06ePo0zTpk1Vt25d/fDDD26JERVTTk6Oli9frjNnzqhTp070HRTJxIkTNXDgQJd+IvHdgwvbs2ePIiMjVb9+fQ0fPlwHDhyQRN/BhX3yySdq3769brzxRoWFhalNmzZ6/fXXHev37duno0ePuvShoKAgdezYkT4EF5mZmXrnnXc0evRoWSyWCv39Q5JUCna7Xffee6+6dOmiyy67TJJ09OhR+fj4qEaNGi5la9euraNHj7ohSlQ0v/76q/z9/eXr66vx48fro48+UvPmzek7uKDly5frl19+0ezZs/Oso/+gMB07dtTixYu1atUqLViwQPv27VPXrl2VkpJC38EF/fXXX1qwYIEaNWqkr776ShMmTNDdd9+tt956S5Ic/aR27dou9ehDON/KlSuVlJSkUaNGSarYv7u83Np6JTdx4kTt3LnTZVw3cCFNmjTRtm3blJycrPfff18jR47Uhg0b3B0WKriDBw/qnnvu0erVq+Xn5+fucFDJDBgwwPH/li1bqmPHjoqJidG///1vWa1WN0aGysBut6t9+/Z68sknJUlt2rTRzp07tXDhQo0cOdLN0aEyeeONNzRgwABFRka6O5QL4kxSCU2aNEmfffaZ1q1bpzp16jiWh4eHKzMzU0lJSS7ljx07pvDw8HKOEhWRj4+PGjZsqHbt2mn27Nlq1aqVXnjhBfoOCrV161YdP35cbdu2lZeXl7y8vLRhwwa9+OKL8vLyUu3atek/KLIaNWqocePG2rt3L989uKCIiAg1b97cZVmzZs0cQzZz+8n5M5LRh+AsPj5ea9as0R133OFYVpG/f0iSiskYo0mTJumjjz7SN998o9jYWJf17dq1k7e3t9auXetYtmvXLh04cECdOnUq73BRCdjtdmVkZNB3UKjevXvr119/1bZt2xyP9u3ba/jw4Y7/039QVKmpqYqLi1NERATfPbigLl265Lndye7duxUTEyNJio2NVXh4uEsfstls+umnn+hDcFi0aJHCwsI0cOBAx7IK/f3j1mkjKqEJEyaYoKAgs379enPkyBHH4+zZs44y48ePN3Xr1jXffPON2bJli+nUqZPp1KmTG6NGRfHggw+aDRs2mH379pkdO3aYBx980FgsFvP1118bY+g7KB7n2e2Mof+gYPfdd59Zv3692bdvn9m0aZPp06ePCQkJMcePHzfG0HdQuJ9//tl4eXmZf/7zn2bPnj1m6dKlplq1auadd95xlHnqqadMjRo1zMcff2x27Nhhrr/+ehMbG2vS0tLcGDkqipycHFO3bl3zj3/8I8+6ivr9Q5JUTJLyfSxatMhRJi0tzfzf//2fqVmzpqlWrZoZPHiwOXLkiPuCRoUxevRoExMTY3x8fExoaKjp3bu3I0Eyhr6D4jk/SaL/oCA333yziYiIMD4+PiYqKsrcfPPNZu/evY719B1cyKeffmouu+wy4+vra5o2bWpee+01l/V2u908+uijpnbt2sbX19f07t3b7Nq1y03RoqL56quvjKR8+0RF/f6xGGOMG09kAQAAAECFwjVJAAAAAOCEJAkAAAAAnJAkAQAAAIATkiQAAAAAcEKSBAAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACckSQCAMrV48WLVqFHD3WFUGT169NC9995brDoWi0UrV64scP369etlsViUlJRUqtgAoLIgSQKAi+SHH36Qp6enBg4c6O5QyoXFYnE8qlevrkaNGmnUqFHaunWrS7mbb75Zu3fvLtI2K1tCdaHkoqhmzJghi8Wi8ePHuyzftm2bLBaL9u/fX+Rtffjhh5o1a1apYwKAqowkCQAukjfeeEN33XWXvv32Wx0+fLhM2zLGKDs7u0zbKIpFixbpyJEj+u233zR//nylpqaqY8eOevvttx1lrFarwsLC3Bhl5eDn56c33nhDe/bsKdV2atWqpYCAgIsUVdnKzMx0dwgAkC+SJAC4CFJTU/Xee+9pwoQJGjhwoBYvXuxYd+utt+rmm292KZ+VlaWQkBBHMmG32zV79mzFxsbKarWqVatWev/99x3lc4c7ffnll2rXrp18fX313XffKS4uTtdff71q164tf39/dejQQWvWrHFp68iRIxo4cKCsVqtiY2O1bNky1atXT/PmzXOUSUpK0h133KHQ0FAFBgaqV69e2r59+wVfd40aNRQeHq569eqpb9++ev/99zV8+HBNmjRJp0+flpT37ND27dvVs2dPBQQEKDAwUO3atdOWLVu0fv163X777UpOTnacoZoxY4YkacmSJWrfvr0CAgIUHh6uW2+9VcePH8+zf9auXav27durWrVq6ty5s3bt2uUS76effqoOHTrIz89PISEhGjx4sGNdRkaG7r//fkVFRal69erq2LGj1q9fX+Brr1evniRp8ODBslgsjueStGDBAjVo0EA+Pj5q0qSJlixZcsF92aRJE/Xs2VMPP/xwoeV27typAQMGyN/fX7Vr19aIESN04sQJx/rzh9sV5f2XpBMnTmjw4MGqVq2aGjVqpE8++SRP25s2bVLLli3l5+enK6+8Ujt37nRZ/8EHH6hFixby9fVVvXr19Nxzz7msr1evnmbNmqW///3vCgwM1Lhx45SZmalJkyYpIiJCfn5+iomJ0ezZsy+4vwCgTBkAQKm98cYbpn379sYYYz799FPToEEDY7fbjTHGfPbZZ8ZqtZqUlBRH+U8//dRYrVZjs9mMMcY88cQTpmnTpmbVqlUmLi7OLFq0yPj6+pr169cbY4xZt26dkWRatmxpvv76a7N3715z8uRJs23bNrNw4ULz66+/mt27d5tHHnnE+Pn5mfj4eEdbffr0Ma1btzY//vij2bp1q+nevbuxWq3m+eefdylz3XXXmc2bN5vdu3eb++67zwQHB5uTJ08W+JolmY8++ijP8v/85z9GknnvvfeMMcYsWrTIBAUFOda3aNHC3HbbbeaPP/4wu3fvNv/+97/Ntm3bTEZGhpk3b54JDAw0R44cMUeOHHHsszfeeMN88cUXJi4uzvzwww+mU6dOZsCAAY5t5u6fjh07mvXr15vffvvNdO3a1XTu3NlR5rPPPjOenp7mscceM7///rvZtm2befLJJx3r77jjDtO5c2fz7bffmr1795pnn33W+Pr6mt27d+f7+o8fP24kmUWLFpkjR46Y48ePG2OM+fDDD423t7eZP3++2bVrl3nuueeMp6en+eabbwrcl9OnTzetWrUyW7duNR4eHmbz5s0u+3Lfvn3GGGNOnz5tQkNDzbRp08wff/xhfvnlF3P11Vebnj17OrbVvXt3c8899zieF+X9l2Tq1Kljli1bZvbs2WPuvvtu4+/v73j/c/dvs2bNzNdff2127Nhhrr32WlOvXj2TmZlpjDFmy5YtxsPDwzz++ONm165dZtGiRcZqtZpFixY52omJiTGBgYFmzpw5Zu/evY79HB0dbb799luzf/9+s3HjRrNs2bIC9xUAlAeSJAC4CDp37mzmzZtnjDEmKyvLhISEmHXr1rk8f/vttx3lb7nlFnPzzTcbY4xJT0831apVM99//73LNseMGWNuueUWY8z/DlJXrlx5wVhatGhhXnrpJWOMMX/88YeR5DjoNsaYPXv2GEmOg+SNGzeawMBAk56e7rKdBg0amFdffbXAdgpKktLS0owk8/TTTxtj8iZJAQEBZvHixflu8/yyBdm8ebOR5EiicvfPmjVrHGU+//xzI8mkpaUZY4zp1KmTGT58eL7bi4+PN56eniYhIcFlee/evc20adMKjCO/fdC5c2czduxYl2U33nijueaaawrcTm6SZIwxw4YNM7169TLG5E2SZs2aZfr27etS9+DBg0aS2bVrlzHGNUkqyvuf+zoeeeQRx/PU1FQjyXz55ZfGmP/t3+XLlzvKnDx50litVkcyfOutt5qrr77aJbYHHnjANG/e3PE8JibGDBo0yKXMXXfdZXr16uX4owIAVAQMtwOAUtq1a5d+/vln3XLLLZIkLy8v3XzzzXrjjTccz2+66SYtXbpUknTmzBl9/PHHGj58uCRp7969Onv2rK6++mr5+/s7Hm+//bbi4uJc2mrfvr3L89TUVN1///1q1qyZatSoIX9/f/3xxx86cOCAIzYvLy+1bdvWUadhw4aqWbOm4/n27duVmpqq4OBgl/b37duXp/2iMMZIOjepQX6mTJmiO+64Q3369NFTTz1VpDa2bt2q6667TnXr1lVAQIC6d+8uSY7Xmatly5aO/0dEREiSY1jetm3b1Lt373y3/+uvvyonJ0eNGzd22QcbNmwo9j74448/1KVLF5dlXbp00R9//FGk+k888YQ2btyor7/+Os+67du3a926dS4xNm3aVJLyjbMo738u531XvXp1BQYGugxplKROnTo5/l+rVi01adLE8boKet179uxRTk6OY9n5fXjUqFHatm2bmjRporvvvjvf1w0A5c3L3QEAQGX3xhtvKDs7W5GRkY5lxhj5+vrq5ZdfVlBQkIYPH67u3bvr+PHjWr16taxWq/r37y/pXKIjSZ9//rmioqJctu3r6+vyvHr16i7P77//fq1evVpz5sxRw4YNZbVadcMNNxTrgvjU1FRFRETke/1NSWaayz1ojo2NzXf9jBkzdOutt+rzzz/Xl19+qenTp2v58uUu1wc5O3PmjPr166d+/fpp6dKlCg0N1YEDB9SvX788r9Pb29vx/9wkzW63Szo3gURBUlNT5enpqa1bt8rT09Nlnb+//wVe8cXVoEEDjR07Vg8++KAj0c6Vmpqq6667Tk8//XSeerlJYUk57zvp3P7L3XcX0/l9uG3bttq3b5++/PJLrVmzRjfddJP69Onjck0eAJQ3kiQAKIXs7Gy9/fbbeu6559S3b1+XdYMGDdK7776r8ePHq3PnzoqOjtZ7772nL7/8UjfeeKPjoLR58+by9fXVgQMHHGdIimrTpk0aNWqUI8FITU11mS66SZMmys7O1n/+8x+1a9dO0rkzV7mTKkjnDlKPHj0qLy8vl8kHSmrevHkKDAxUnz59CizTuHFjNW7cWJMnT9Ytt9yiRYsWafDgwfLx8XE56yBJf/75p06ePKmnnnpK0dHRkqQtW7YUO66WLVtq7dq1uv322/Osa9OmjXJycnT8+HF17dq1yNv09vbOE2+zZs20adMmjRw50rFs06ZNat68eZG3+9hjj6lBgwZavny5y/K2bdvqgw8+UL169eTldeFf4UV5/4vjxx9/VN26dSVJp0+f1u7du9WsWTNJ/3vdzjZt2qTGjRvnSTzPFxgYqJtvvlk333yzbrjhBvXv31+nTp1SrVq1ShQnAJQWSRIAlMJnn32m06dPa8yYMQoKCnJZN3ToUL3xxhuOe9/ceuutWrhwoXbv3q1169Y5ygUEBOj+++/X5MmTZbfbddVVVyk5OVmbNm1SYGCgy8H2+Ro1aqQPP/xQ1113nSwWix599FGXv/43bdpUffr00bhx47RgwQJ5e3vrvvvuk9VqdZxp6dOnjzp16qRBgwbpmWeeUePGjXX48GF9/vnnGjx4cJ7hUc6SkpJ09OhRZWRkaPfu3Xr11Ve1cuVKvf322/mehUpLS9MDDzygG264QbGxsTp06JA2b96soUOHSjo3+1lqaqrWrl2rVq1aqVq1aqpbt658fHz00ksvafz48dq5c2eJ7gM0ffp09e7dWw0aNNCwYcOUnZ2tL774Qv/4xz/UuHFjDR8+XH//+9/13HPPqU2bNkpMTNTatWvVsmXLAu99Va9ePa1du1ZdunSRr6+vatasqQceeEA33XST2rRpoz59+ujTTz/Vhx9+mGfWwcLUrl1bU6ZM0bPPPuuyfOLEiXr99dd1yy23aOrUqapVq5b27t2r5cuX61//+leeZKQo739xPP744woODlbt2rX18MMPKyQkRIMGDZIk3XffferQoYNmzZqlm2++WT/88INefvllvfLKK4Vuc+7cuYqIiFCbNm3k4eGhFStWKDw8vFLdLwvAJcjdF0UBQGV27bXXFnhB/k8//WQkme3btxtjjPn999+NJBMTE5PnInW73W7mzZtnmjRpYry9vU1oaKjp16+f2bBhgzHmfxfOnz592qXevn37TM+ePY3VajXR0dHm5ZdfzjO72eHDh82AAQOMr6+viYmJMcuWLTNhYWFm4cKFjjI2m83cddddJjIy0nh7e5vo6GgzfPhwc+DAgQJfuyTHw8/PzzRo0MCMHDnSbN261aWc82QMGRkZZtiwYSY6Otr4+PiYyMhIM2nSJMfkCsYYM378eBMcHGwkmenTpxtjjFm2bJmpV6+e8fX1NZ06dTKffPKJkWT+85//FLh/zp/0wBhjPvjgA9O6dWvj4+NjQkJCzJAhQxzrMjMzzWOPPWbq1atnvL29TUREhBk8eLDZsWNHgfvgk08+MQ0bNjReXl4mJibGsfyVV14x9evXN97e3qZx48Yuk3bkx3nihlzJyckmJCQkz2vYvXu3GTx4sKlRo4axWq2madOm5t5773X0qZK8/8pnAoqgoCDHzHS5+/fTTz81LVq0MD4+PuaKK65w9O1c77//vmnevLnx9vY2devWNc8++6zL+piYGJcJI4wx5rXXXjOtW7c21atXN4GBgaZ3797ml19+KXR/AUBZsxjz3ytsAQBVwqFDhxQdHa01a9YUOJEBLl28/wBwYSRJAHCJ++abb5SamqrLL79cR44c0dSpU5WQkKDdu3fnuVgflx7efwAoPq5JAoBLXFZWlh566CH99ddfCggIUOfOnbV06VIOkKsI3n8AKD7OJAEAAACAE24mCwAAAABOSJIAAAAAwAlJEgAAAAA4IUkCAAAAACckSQAAAADghCQJAAAAAJyQJAEAAACAE5IkAAAAAHDy/2PqhIB86Z+4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 8: Prediction Pipeline (Phase 4)**\n",
        "\n",
        "* **What it does:** Loads the model and tests it on new images."
      ],
      "metadata": {
        "id": "kXaK3OcWglju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from pathlib import Path\n",
        "import csv\n",
        "\n",
        "# Image preprocessing configuration (must match training)\n",
        "IMAGE_SIZE = 128\n",
        "\n",
        "# Class mapping\n",
        "CLASS_MAPPING = {\n",
        "    0: 'cardboard', 1: 'glass', 2: 'metal',\n",
        "    3: 'paper', 4: 'plastic', 5: 'trash', 6: 'unknown'\n",
        "}\n",
        "\n",
        "\n",
        "def load_model_and_scaler(bestModelPath, model_type):\n",
        "    \"\"\"\n",
        "    Load the trained SVM/KNN model, feature scaler, and CNN backbone\n",
        "\n",
        "    Args:\n",
        "        bestModelPath: Path to the saved_models directory\n",
        "\n",
        "    Returns:\n",
        "        model: Trained classifier (SVM or KNN)\n",
        "        scaler: Feature scaler\n",
        "        cnn_model: CNN feature extractor\n",
        "        device: torch device\n",
        "        transform: Image transformation pipeline\n",
        "    \"\"\"\n",
        "    model_dir = Path(bestModelPath)\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "    # Load CNN backbone for feature extraction\n",
        "    print(\"[INFO] Loading CNN feature extractor...\")\n",
        "    cnn_path = model_dir / 'cnn_feature_extractor.pth'\n",
        "\n",
        "    if not cnn_path.exists():\n",
        "        raise FileNotFoundError(f\"CNN model not found at {cnn_path}\")\n",
        "\n",
        "    cnn_model = models.resnet50(pretrained=False)\n",
        "    cnn_model.fc = nn.Identity()  # Remove classifier head\n",
        "\n",
        "    state = torch.load(cnn_path, map_location=device)\n",
        "    cnn_model.load_state_dict(state, strict=False)\n",
        "    cnn_model = cnn_model.to(device)\n",
        "    cnn_model.eval()\n",
        "\n",
        "    # Load feature scaler\n",
        "    print(\"[INFO] Loading feature scaler...\")\n",
        "    scaler_path = model_dir / 'feature_scaler.pkl'\n",
        "\n",
        "    if not scaler_path.exists():\n",
        "        raise FileNotFoundError(f\"Feature scaler not found at {scaler_path}\")\n",
        "\n",
        "    with open(scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "\n",
        "    # Load classifier model (try SVM first, then KNN)\n",
        "    model = None\n",
        "    model_type = model_type\n",
        "\n",
        "    if model_type == 'svm':\n",
        "        svm_path = model_dir / 'svm_model.pkl'\n",
        "        print(\"[INFO] Loading SVM model...\")\n",
        "        with open(svm_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        model_type = 'svm'\n",
        "\n",
        "        # Load SVM config for threshold\n",
        "        config_path = model_dir / 'svm_config.json'\n",
        "        if config_path.exists():\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            threshold = config.get('optimal_threshold', 0.6)\n",
        "        else:\n",
        "            threshold = 0.6\n",
        "    elif model_type == 'knn':\n",
        "        knn_path = model_dir / 'knn_model.pkl'\n",
        "        print(\"[INFO] Loading KNN model...\")\n",
        "        with open(knn_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        model_type = 'knn'\n",
        "        threshold = None  # KNN doesn't use threshold\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No model found in {model_dir}\")\n",
        "\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"Failed to load any model\")\n",
        "\n",
        "    print(f\"[OK] Loaded {model_type.upper()} model successfully\")\n",
        "\n",
        "    # Define image transformation pipeline\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    return model, scaler, cnn_model, device, transform, model_type, threshold\n",
        "\n",
        "\n",
        "def load_images_from_folder(dataFilePath):\n",
        "    \"\"\"\n",
        "    Load all images from the given folder path\n",
        "\n",
        "    Args:\n",
        "        dataFilePath: Path to folder containing images\n",
        "\n",
        "    Returns:\n",
        "        images: List of (image_array, filename) tuples\n",
        "    \"\"\"\n",
        "    folder_path = Path(dataFilePath)\n",
        "\n",
        "    if not folder_path.exists():\n",
        "        raise FileNotFoundError(f\"Folder not found: {dataFilePath}\")\n",
        "\n",
        "    # Supported image extensions\n",
        "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
        "\n",
        "    # Load all images\n",
        "    images = []\n",
        "    image_files = []\n",
        "\n",
        "    for file_path in sorted(folder_path.iterdir()):\n",
        "        if file_path.suffix.lower() in valid_extensions:\n",
        "            try:\n",
        "                # Read image using OpenCV\n",
        "                img = cv2.imread(str(file_path))\n",
        "\n",
        "                if img is None:\n",
        "                    print(f\"[WARNING] Failed to load: {file_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR to RGB\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                images.append(img)\n",
        "                image_files.append(file_path.name)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[WARNING] Error loading {file_path.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"[INFO] Loaded {len(images)} images from {dataFilePath}\")\n",
        "\n",
        "    return images, image_files\n",
        "\n",
        "\n",
        "def extract_features(images, cnn_model, scaler, transform, device):\n",
        "    \"\"\"\n",
        "    Extract CNN features from images\n",
        "\n",
        "    Args:\n",
        "        images: List of image arrays\n",
        "        cnn_model: CNN feature extractor\n",
        "        scaler: Feature scaler\n",
        "        transform: Image transformation pipeline\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        features: Numpy array of extracted and scaled features\n",
        "    \"\"\"\n",
        "    features_list = []\n",
        "\n",
        "    print(\"[INFO] Extracting features...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, img in enumerate(images):\n",
        "            try:\n",
        "                # Validate image\n",
        "                if img is None or img.size == 0:\n",
        "                    features_list.append(np.zeros(2048))\n",
        "                    continue\n",
        "\n",
        "                if img.ndim != 3 or img.shape[2] != 3:\n",
        "                    features_list.append(np.zeros(2048))\n",
        "                    continue\n",
        "\n",
        "                # Ensure uint8\n",
        "                img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "                # Transform image\n",
        "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "                # Extract features\n",
        "                feature_vector = cnn_model(img_tensor)\n",
        "                feature_vector = feature_vector.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "                # Normalize with scaler\n",
        "                feature_vector = scaler.transform(feature_vector)\n",
        "\n",
        "                features_list.append(feature_vector.flatten())\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[WARNING] Error extracting features for image {i}: {e}\")\n",
        "                features_list.append(np.zeros(2048))\n",
        "\n",
        "    features = np.array(features_list)\n",
        "    print(f\"[OK] Extracted features with shape: {features.shape}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def make_predictions(model, features, model_type, threshold=None):\n",
        "    \"\"\"\n",
        "    Make predictions using the loaded model\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier\n",
        "        features: Feature array\n",
        "        model_type: 'svm' or 'knn'\n",
        "        threshold: Confidence threshold for SVM (optional)\n",
        "\n",
        "    Returns:\n",
        "        predictions: List of predicted class names\n",
        "        confidences: List of confidence scores\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "\n",
        "    print(\"[INFO] Making predictions...\")\n",
        "\n",
        "    if model_type == 'svm':\n",
        "        # SVM with confidence thresholding\n",
        "        probabilities = model.predict_proba(features)\n",
        "\n",
        "        for prob in probabilities:\n",
        "            max_prob = prob.max()\n",
        "            class_id = prob.argmax()\n",
        "\n",
        "            # Apply rejection threshold\n",
        "            # REJECTION MECHANISM\n",
        "            # If the model is not confident enough, force 'unknown'\n",
        "            if threshold is not None and max_prob < threshold:\n",
        "                class_id = 6  # Unknown class\n",
        "                print(f\"   -> Rejected sample (Conf: {max_prob:.2f} < {threshold})\")\n",
        "\n",
        "            class_name = CLASS_MAPPING.get(class_id, 'unknown')\n",
        "            predictions.append(class_name)\n",
        "            confidences.append(max_prob)\n",
        "    # --- KNN STRATEGY: DISTANCE THRESHOLD ---\n",
        "    elif model_type == 'knn':\n",
        "        # 1. Get neighbors and distances\n",
        "        # kneighbors returns (distances, indices) for the k nearest neighbors\n",
        "        # This tells us HOW FAR the test image is from the training images\n",
        "        distances, indices = model.kneighbors(features)\n",
        "\n",
        "        # 2. Calculate the average distance to the k neighbors\n",
        "        mean_distances = distances.mean(axis=1)\n",
        "\n",
        "        # 3. Predict based on voting (standard KNN)\n",
        "        raw_predictions = model.predict(features)\n",
        "\n",
        "        # 4. REJECTION MECHANISM\n",
        "        # If threshold is None, default to a safe value (e.g. 15.0 - requires tuning)\n",
        "        # Higher distance = Image is very different from training data\n",
        "        dist_threshold = threshold if threshold is not None else 15.0\n",
        "        # dist_threshold = the ouput number of the previous cell\n",
        "\n",
        "        for i, (pred_id, dist) in enumerate(zip(raw_predictions, mean_distances)):\n",
        "            final_id = pred_id\n",
        "\n",
        "            # If the image is too far away from known samples -> Unknown\n",
        "            if dist > dist_threshold:\n",
        "                final_id = 6  # Force Unknown ID\n",
        "                print(f\"   -> Rejected sample (Dist: {dist:.2f} > {dist_threshold})\")\n",
        "\n",
        "            # Create a confidence score (inverse of distance) for display\n",
        "            # Closer distance = Higher confidence\n",
        "            confidence = 1.0 / (1.0 + dist)\n",
        "\n",
        "            class_name = CLASS_MAPPING.get(final_id, 'unknown')\n",
        "            predictions.append(class_name)\n",
        "            confidences.append(confidence)\n",
        "\n",
        "  # elif model_type == 'knn':\n",
        "  #     # KNN prediction\n",
        "  #     if hasattr(model, 'predict_proba'):\n",
        "  #         probabilities = model.predict_proba(features)\n",
        "\n",
        "  #         for prob in probabilities:\n",
        "  #             max_prob = prob.max()\n",
        "  #             class_id = prob.argmax()\n",
        "  #             class_name = CLASS_MAPPING.get(class_id, 'unknown')\n",
        "  #             predictions.append(class_name)\n",
        "  #             confidences.append(max_prob)\n",
        "  #     else:\n",
        "  #         # KNN without probabilities\n",
        "  #         class_ids = model.predict(features)\n",
        "  #         predictions = [CLASS_MAPPING.get(cid, 'unknown') for cid in class_ids]\n",
        "  #         confidences = [1.0] * len(predictions)  # Placeholder\n",
        "\n",
        "    else:\n",
        "        # Simple prediction without threshold\n",
        "        class_ids = model.predict(features)\n",
        "        predictions = [CLASS_MAPPING.get(cid, 'unknown') for cid in class_ids]\n",
        "\n",
        "        # Try to get confidence scores\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            probabilities = model.predict_proba(features)\n",
        "            confidences = [prob.max() for prob in probabilities]\n",
        "        else:\n",
        "            confidences = [1.0] * len(predictions)\n",
        "\n",
        "    print(f\"[OK] Generated {len(predictions)} predictions\")\n",
        "\n",
        "    return predictions, confidences\n",
        "\n",
        "\n",
        "def predict(dataFilePath, bestModelPath, model_type, manual_threshold=None):\n",
        "    \"\"\"\n",
        "    Main prediction function with CSV export\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"PREDICTION PIPELINE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Data folder: {dataFilePath}\")\n",
        "        print(f\"Model path: {bestModelPath}\")\n",
        "        print()\n",
        "\n",
        "        # Step 1: Load model and preprocessing components\n",
        "        model, scaler, cnn_model, device, transform, model_type, loaded_threshold = load_model_and_scaler(bestModelPath, model_type)\n",
        "\n",
        "        # PRIORITY LOGIC:\n",
        "        # 1. Use manual_threshold if provided (for testing)\n",
        "        # 2. Else use loaded_threshold from config file\n",
        "        # 3. Else default to 0.6\n",
        "        if manual_threshold is not None:\n",
        "            final_threshold = manual_threshold\n",
        "        elif loaded_threshold is not None:\n",
        "            final_threshold = loaded_threshold\n",
        "        else:\n",
        "            final_threshold = 0.6\n",
        "\n",
        "        print(f\"[CONFIG] Using Rejection Threshold: {final_threshold} for {model_type}\")\n",
        "\n",
        "        # Step 2: Load images from folder\n",
        "        images, image_files = load_images_from_folder(dataFilePath)\n",
        "\n",
        "        if len(images) == 0:\n",
        "            print(\"[WARNING] No valid images found in folder\")\n",
        "            return []\n",
        "\n",
        "        # Step 3: Extract features\n",
        "        features = extract_features(images, cnn_model, scaler, transform, device)\n",
        "\n",
        "        # Step 4: Make predictions\n",
        "        predictions, confidences = make_predictions(model, features, model_type, final_threshold)\n",
        "\n",
        "        # Step 5: Display results\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"PREDICTION RESULTS\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"{'Image':<30} {'Prediction':<15} {'Confidence':<10}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for img_name, pred, conf in zip(image_files, predictions, confidences):\n",
        "            print(f\"{img_name:<30} {pred:<15} {conf:.4f}\")\n",
        "\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        # =========================================================\n",
        "        # Step 6: Save Results to CSV [NEW ADDITION]\n",
        "        # =========================================================\n",
        "        # Save in a 'results' folder next to your 'saved_models' folder\n",
        "        results_dir = Path('/content/results')\n",
        "        results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        csv_filename = f\"prediction_results_{model_type}.csv\"\n",
        "        csv_path = results_dir / csv_filename\n",
        "\n",
        "        print(f\"[SAVING] Saving results to CSV: {csv_path}\")\n",
        "\n",
        "        try:\n",
        "            with open(csv_path, mode='w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                # Write Header\n",
        "                writer.writerow(['Image Name', 'Prediction', 'Confidence', 'Threshold Used'])\n",
        "\n",
        "                # Write Rows\n",
        "                for img_name, pred, conf in zip(image_files, predictions, confidences):\n",
        "                    writer.writerow([img_name, pred, f\"{conf:.4f}\", final_threshold])\n",
        "\n",
        "            print(f\"[OK] CSV saved successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Could not save CSV: {e}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Prediction failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataFilePath = \"/content/drive/MyDrive/testset-E\"\n",
        "    bestModelPath = \"/content/drive/MyDrive/CNN/saved_models\"\n",
        "\n",
        "    # CHANGE THIS VALUE to the one you saw in Cell 7 output\n",
        "    MY_KNN_THRESHOLD = 70.94\n",
        "\n",
        "    # Run prediction\n",
        "    results = predict(dataFilePath, bestModelPath, model_type='svm')\n",
        "    # results = predict(dataFilePath, bestModelPath, model_type='knn', manual_threshold=MY_KNN_THRESHOLD)\n",
        "\n",
        "    print(f\"\\n[SUMMARY] Predicted {len(results)} images\")\n",
        "    print(f\"[RESULTS] {results}\")"
      ],
      "metadata": {
        "id": "cFEv7U5TBRk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67651c7b-f5cb-42b5-e0cb-3ee85cc293fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PREDICTION PIPELINE\n",
            "======================================================================\n",
            "Data folder: /content/drive/MyDrive/testset-E\n",
            "Model path: /content/drive/MyDrive/CNN/saved_models\n",
            "\n",
            "[INFO] Using device: cuda\n",
            "[INFO] Loading CNN feature extractor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading feature scaler...\n",
            "[INFO] Loading SVM model...\n",
            "[OK] Loaded SVM model successfully\n",
            "[CONFIG] Using Rejection Threshold: 0.3 for svm\n",
            "[INFO] Loaded 60 images from /content/drive/MyDrive/testset-E\n",
            "[INFO] Extracting features...\n",
            "[OK] Extracted features with shape: (60, 2048)\n",
            "[INFO] Making predictions...\n",
            "[OK] Generated 60 predictions\n",
            "\n",
            "======================================================================\n",
            "PREDICTION RESULTS\n",
            "======================================================================\n",
            "Image                          Prediction      Confidence\n",
            "----------------------------------------------------------------------\n",
            "(1).jpg                        paper           0.9083\n",
            "(10).jpg                       paper           0.9681\n",
            "(11).jpg                       plastic         0.8013\n",
            "(12).jpg                       plastic         0.8283\n",
            "(13).jpg                       plastic         0.9734\n",
            "(14).jpg                       plastic         0.8649\n",
            "(15).jpg                       glass           0.5900\n",
            "(16).jpg                       plastic         0.5602\n",
            "(17).jpg                       plastic         0.9080\n",
            "(18).jpg                       plastic         0.9593\n",
            "(19).jpg                       plastic         0.8745\n",
            "(2).jpg                        paper           0.9668\n",
            "(20).jpg                       plastic         0.5877\n",
            "(21).jpg                       glass           0.8807\n",
            "(22).jpg                       glass           0.7737\n",
            "(23).jpg                       glass           0.8989\n",
            "(24).jpg                       glass           0.9880\n",
            "(25).jpg                       glass           0.9918\n",
            "(26).jpg                       glass           0.6475\n",
            "(27).jpg                       glass           0.5409\n",
            "(28).jpg                       glass           0.9374\n",
            "(29).jpg                       glass           0.9338\n",
            "(3).jpg                        paper           0.9196\n",
            "(30).jpg                       glass           0.8914\n",
            "(31).jpg                       metal           0.4083\n",
            "(32).jpg                       metal           0.9762\n",
            "(33).jpg                       trash           0.8230\n",
            "(34).jpg                       glass           0.4945\n",
            "(35).jpg                       metal           0.8878\n",
            "(36).jpg                       metal           0.9252\n",
            "(37).jpg                       trash           0.8504\n",
            "(38).jpg                       metal           0.8772\n",
            "(39).jpg                       metal           0.7683\n",
            "(4).jpg                        paper           0.9728\n",
            "(40).jpg                       metal           0.9818\n",
            "(41).jpg                       cardboard       0.8415\n",
            "(42).jpg                       paper           0.5082\n",
            "(43).jpg                       cardboard       0.9580\n",
            "(44).jpg                       cardboard       0.9992\n",
            "(45).jpg                       cardboard       0.8321\n",
            "(46).jpg                       cardboard       0.9996\n",
            "(47).jpg                       cardboard       0.6493\n",
            "(48).jpg                       cardboard       0.9702\n",
            "(49).jpg                       cardboard       0.7916\n",
            "(5).jpg                        paper           0.7757\n",
            "(50).jpg                       cardboard       0.9206\n",
            "(51).jpg                       trash           0.3360\n",
            "(52).jpg                       trash           0.7867\n",
            "(53).jpg                       glass           0.4419\n",
            "(54).jpg                       trash           0.8732\n",
            "(55).jpg                       trash           0.9951\n",
            "(56).jpg                       trash           0.5279\n",
            "(57).jpg                       trash           0.9737\n",
            "(58).jpg                       trash           0.5843\n",
            "(59).jpg                       paper           0.3248\n",
            "(6).jpg                        paper           0.8522\n",
            "(60).jpg                       glass           0.5558\n",
            "(7).jpg                        paper           0.9814\n",
            "(8).jpg                        paper           0.5295\n",
            "(9).jpg                        paper           0.9637\n",
            "======================================================================\n",
            "\n",
            "[SAVING] Saving results to CSV: /content/results/prediction_results_svm.csv\n",
            "[OK] CSV saved successfully!\n",
            "\n",
            "[SUMMARY] Predicted 60 images\n",
            "[RESULTS] ['paper', 'paper', 'plastic', 'plastic', 'plastic', 'plastic', 'glass', 'plastic', 'plastic', 'plastic', 'plastic', 'paper', 'plastic', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'paper', 'glass', 'metal', 'metal', 'trash', 'glass', 'metal', 'metal', 'trash', 'metal', 'metal', 'paper', 'metal', 'cardboard', 'paper', 'cardboard', 'cardboard', 'cardboard', 'cardboard', 'cardboard', 'cardboard', 'cardboard', 'paper', 'cardboard', 'trash', 'trash', 'glass', 'trash', 'trash', 'trash', 'trash', 'trash', 'paper', 'paper', 'glass', 'paper', 'paper', 'paper']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zb0gQ6yIRitJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}